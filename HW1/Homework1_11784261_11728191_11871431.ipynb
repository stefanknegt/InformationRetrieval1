{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental part\n",
    "\n",
    "1) Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the next section, we first create a list of all combinations of relevances. We use itertools.product which gives all possible combinations of a list in any order. Then we use permutation which gives all the combinations of experiment and production relevances. We use itertools.permutations to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "values = ['N','R','HR'] #possible values of a prediction\n",
    "\n",
    "relevances = [] #relevances contains all combinations of N/R/HR with length 5\n",
    "for r in itertools.product(values, repeat=5):\n",
    "    relevances.append(list(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinations = [] #combinations contains all pairs of relevances\n",
    "for p in itertools.permutations(relevances, 2):\n",
    "    combinations.append(list(p)) #we use this to get rid of the permutations object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'R']]]\n"
     ]
    }
   ],
   "source": [
    "print(combinations[:10]) #show the first 10 combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Step 2: Implement Evaluation Measures (10 points)\n",
    "\n",
    "In the next section we take two assumptions:\n",
    "\n",
    "1) Values for the prediction relevances are N=0, R=1, HR=2\n",
    "\n",
    "2) The amount of relevant predictions (overall) is assumed to be the total amount of relevant (R or HR) docs in the prediction set. So we assume there is no overlap between articles in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "#the first binary evaluation methods: average precision\n",
    "numeric_map = {'N':0, 'R':1, 'HR':2} #we use this numeric map to map N/R/HR to a numeric value.\n",
    "prediction = ['R','HR','N','R','N'] #this is a sample prediction to test functions\n",
    "\n",
    "def count_rel(prediction1,prediction2):\n",
    "    return sum(1 for i in prediction1 if i != 'N') + sum(1 for i in prediction2 if i != 'N')\n",
    "\n",
    "def average_precision(prediction, r):\n",
    "    ap = 0\n",
    "    relevant_preds = 0\n",
    "    for i in range(0,len(prediction)):\n",
    "        if prediction[i] != 'N':\n",
    "            relevant_preds += 1\n",
    "            ap += relevant_preds/(i+1)\n",
    "    return ap/r\n",
    "\n",
    "ap = average_precision(prediction, count_rel(prediction, prediction))\n",
    "print(ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement both multi-graded evaluation methods. \n",
    "\n",
    "The first is nDCG@k which requires a optimal prediction to normalize predictions. Here we will use the total amount of HR/R files to create an optimum prediction. Again we assume there is no overlap in predictions.\n",
    "\n",
    "The second one is ERR, this model does not need any assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53641800576\n"
     ]
    }
   ],
   "source": [
    "#nDCG@K\n",
    "import numpy as np #Numpy is amazing right?\n",
    "\n",
    "def generate_opt(prediction1, prediction2): #generate optimal sequences from two predictions\n",
    "    opt_pred = []\n",
    "    num_hr = sum(1 for i in prediction1 if i == 'HR') + sum(1 for i in prediction2 if i == 'HR')\n",
    "    num_r  = sum(1 for i in prediction1 if i == 'R') + sum(1 for i in prediction2 if i == 'R')\n",
    "    for i in range(min(num_hr,5)): #check if num_hr exceeds 5, fill with HR's\n",
    "        opt_pred.append('HR')\n",
    "    for i in range(min(5-num_hr,num_r)): #check if num_r exceeds the space left, will with R's\n",
    "        opt_pred.append('R')\n",
    "    for i in range(5-len(opt_pred)): #fill the rest with N\n",
    "        opt_pred.append('N')\n",
    "    return opt_pred\n",
    "\n",
    "def dcg_k(numeric_map, prediction, opt_pred, k):\n",
    "    dcg_opt = 0\n",
    "    dcg = 0\n",
    "    for i in range(0,k): #for the range until K, we sum both the optimum and prediction dcg\n",
    "        dcg_opt += (2**numeric_map[opt_pred[i]]-1)/np.log2(1+i+1)\n",
    "        dcg +=(2**numeric_map[prediction[i]]-1)/np.log2(1+i+1)\n",
    "    return dcg/dcg_opt #dcg is normalized compared to the optimum\n",
    "ndcg = dcg_k(numeric_map, prediction, generate_opt(prediction,prediction), 3)\n",
    "print(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.281982421875\n"
     ]
    }
   ],
   "source": [
    "#ERR\n",
    "def ERR(numeric_map, prediction):\n",
    "    err = 0\n",
    "    max_val = 2**max(list(numeric_map.values()))\n",
    "    thetas = [(2**numeric_map[p]-1)/max_val for p in prediction]\n",
    "    for i in range(0,len(prediction)):\n",
    "        prod_val = 1\n",
    "        for j in range(0,i):\n",
    "            prod_val *= (1-thetas[j])*thetas[i]\n",
    "        prod_val *= 1/(i+1)\n",
    "        err += prod_val\n",
    "    return err\n",
    "err = ERR(numeric_map, prediction)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Step 3: Calculate the ð›¥measure (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'N', 'N', 'N', 'R'] ['HR', 'N', 'N', 'N', 'N']\n",
      "The average prec. scores are  0.4666666666666666 0.3333333333333333  for experiment and production respectively!\n",
      "The ERR scores are  1.0005859375 1.0  for experiment and production respectively!\n",
      "The NDCG scores @ k= 5  are:  0.33572413233 0.726228761795  for experiment and production respectively!!!\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "def check_performance(s):\n",
    "    prediction_e = s[0]\n",
    "    prediction_p = s[1]\n",
    "    print(prediction_e, prediction_p)\n",
    "    r = count_rel(prediction_e, prediction_p)\n",
    "    ap_e, ap_p = average_precision(prediction_e, r), average_precision(prediction_p, r)\n",
    "    print('The average prec. scores are ',ap_e,ap_p,' for experiment and production respectively!')\n",
    "    ERR_e, ERR_p = ERR(numeric_map, prediction_e), ERR(numeric_map, prediction_p)\n",
    "    print('The ERR scores are ',ERR_e,ERR_p,' for experiment and production respectively!')\n",
    "    opt_prediction = generate_opt(prediction_e,prediction_p)\n",
    "    ndcg_e = dcg_k(numeric_map, prediction_e, opt_prediction, k)\n",
    "    ndcg_p = dcg_k(numeric_map, prediction_p, opt_prediction, k)\n",
    "    print('The NDCG scores @ k=',k,' are: ',ndcg_e, ndcg_p,' for experiment and production respectively!!!')\n",
    "\n",
    "check_performance(combinations[20005])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Implement Interleaving (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6, 1, 7, 2, 8, 3, 9, 4, 10], 'Tie')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_A_first(): #This is a function that determines is ranking A goes first or not\n",
    "    A = np.random.uniform() # Take a random uniform number between 0 and 1    \n",
    "    if A > 0.5: \n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def balanced_interleaving(s,clicks):\n",
    "    \n",
    "    ranking_A = s[0]\n",
    "    ranking_B = s[1]\n",
    "    \n",
    "    #print (\"ranking A is\",ranking_A)\n",
    "    #print (\"ranking B is\",ranking_B)\n",
    "    \n",
    "    # Initialize\n",
    "    I = []\n",
    "    k_a, k_b = 0,0\n",
    "        \n",
    "    A_first = get_A_first() #Find out if A or B goes first\n",
    "        \n",
    "    # This code just follows the pseudo code from the slides\n",
    "    while k_a+1 <= len(ranking_A) and k_b+1 <= len(ranking_B):\n",
    "        if (k_a < k_b) or ((k_a == k_b) and A_first):\n",
    "            if ranking_A[k_a] not in I:\n",
    "                I.append(ranking_A[k_a])\n",
    "            k_a += 1\n",
    "            \n",
    "        else:\n",
    "            if ranking_B[k_b] not in I:\n",
    "                I.append(ranking_B[k_b])\n",
    "            k_b += 1\n",
    "            \n",
    "    \n",
    "    clicks_A = 0\n",
    "    clicks_B = 0 \n",
    "    \n",
    "    for click in clicks:\n",
    "        if click in ranking_A:\n",
    "            clicks_A += 1\n",
    "        elif click in ranking_B:\n",
    "            clicks_B += 1\n",
    "        \n",
    "            \n",
    "    if clicks_A > clicks_B:\n",
    "        return I, \"A\"\n",
    "    elif clicks_B > clicks_A:\n",
    "        return I, \"B\"\n",
    "    else: \n",
    "        return I, \"Tie\"\n",
    "            \n",
    "test_set = [[1,2,3,4,5],[6,7,8,9,10]]\n",
    "clicks = [1,6]\n",
    "\n",
    "balanced_interleaving(test_set,clicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Step 5: Implement User Clicks Simulation (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read search query data\n",
    "import csv\n",
    "import re\n",
    "\n",
    "answers = []\n",
    "clicks = []\n",
    "click = []\n",
    "last_type = 'C'\n",
    "with open('YandexRelPredChallenge.txt') as f:\n",
    "    for line in f:\n",
    "        vals = re.split(r'\\t+', line.rstrip())\n",
    "        line_type = vals[2] #we look at the type of data line\n",
    "        if line_type == 'Q': #if type is query, we append the query.\n",
    "            if len(click) > 0: #we append clicks of last query before we go further\n",
    "                clicks.append(click)\n",
    "                click = []\n",
    "            answers.append(list(map(int, vals[5:])))\n",
    "        if last_type == 'Q' and line_type == 'Q': #If last type also was query there are no clicks  \n",
    "            clicks.append([])\n",
    "        elif line_type == 'C':\n",
    "            click.append(int(vals[3]))\n",
    "        last_type = vals[2]\n",
    "    clicks.append(click) #we shall not forget the last click sequence...\n",
    "print('Some sample answers and clicks:')\n",
    "print(answers[:5])\n",
    "print(clicks[:5])\n",
    "print('We have ',len(clicks),' answers/click sequences in total!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
