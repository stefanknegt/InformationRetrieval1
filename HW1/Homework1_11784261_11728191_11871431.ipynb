{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental part\n",
    "\n",
    "1) Step 1: Simulate Rankings of Relevance for E and P (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "values = ['N','R','HR']\n",
    "\n",
    "relevances = [] #relevances contains all combinations of N/R/HR with length 5\n",
    "for r in itertools.product(values, repeat=5):\n",
    "    relevances.append(list(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [] #combinations contains all pairs of relevances\n",
    "for p in itertools.permutations(relevances, 2):\n",
    "    combinations.append(list(p)) #we use this to get rid of the permutations object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'R']]]\n"
     ]
    }
   ],
   "source": [
    "print(combinations[:10]) #show the first 10 combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Step 2: Implement Evaluation Measures (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "#the first binary evaluation methods: average precision\n",
    "numeric_map = {'N':0, 'R':1, 'HR':2}\n",
    "prediction = ['R','HR','N','R','N']\n",
    "\n",
    "def count_rel(prediction1,prediction2):\n",
    "    return max(sum(1 for i in prediction1 if i != 'N'), sum(1 for i in prediction2 if i != 'N'))\n",
    "\n",
    "def average_precision(prediction, r):\n",
    "    ap = 0\n",
    "    relevant_preds = 0\n",
    "    for i in range(0,len(prediction)):\n",
    "        if prediction[i] != 'N':\n",
    "            relevant_preds += 1\n",
    "            ap += relevant_preds/(i+1)\n",
    "    return ap/r\n",
    "\n",
    "ap = average_precision(prediction, count_rel(prediction, prediction))\n",
    "print(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.700275587648\n"
     ]
    }
   ],
   "source": [
    "#DCG@K\n",
    "import numpy as np\n",
    "\n",
    "def generate_opt(prediction1, prediction2): #generate optimal sequences from two predictions\n",
    "    opt_pred = []\n",
    "    num_hr = max(sum(1 for i in prediction1 if i == 'HR'), sum(1 for i in prediction2 if i == 'HR'))\n",
    "    num_r  = max(sum(1 for i in prediction1 if i == 'R'), sum(1 for i in prediction2 if i == 'R'))\n",
    "    for i in range(num_hr):\n",
    "        opt_pred.append('HR')\n",
    "    for i in range(min(5-num_hr,num_r)):\n",
    "        opt_pred.append('R')\n",
    "    for i in range(5-len(opt_pred)):\n",
    "        opt_pred.append('N')\n",
    "    return opt_pred\n",
    "\n",
    "def dcg_k(numeric_map, prediction, opt_pred, k):\n",
    "    dcg_opt = 0\n",
    "    dcg = 0\n",
    "    for i in range(0,k):\n",
    "        dcg_opt += (2**numeric_map[opt_pred[i]]-1)/np.log2(1+i+1)\n",
    "        dcg +=(2**numeric_map[prediction[i]]-1)/np.log2(1+i+1)\n",
    "    return dcg/dcg_opt\n",
    "ndcg = dcg_k(numeric_map, prediction, generate_opt(prediction,prediction), 3)\n",
    "print(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.28125\n"
     ]
    }
   ],
   "source": [
    "#ERR\n",
    "def ERR(numeric_map, prediction):\n",
    "    err = 0\n",
    "    max_val = 2**max(list(numeric_map.values()))\n",
    "    thetas = [(2**numeric_map[p]-1)/max_val for p in prediction]\n",
    "    for i in range(0,2):#len(prediction)):\n",
    "        prod_val = 1\n",
    "        for j in range(0,i):\n",
    "            prod_val *= (1-thetas[j])*thetas[i]\n",
    "        prod_val *= 1/(i+1)\n",
    "        err += prod_val\n",
    "    return err\n",
    "err = ERR(numeric_map, prediction)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Step 3: Calculate the ùõ•measure (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'N', 'N', 'N', 'R'] ['HR', 'N', 'N', 'N', 'N']\n",
      "The average prec. scores are  0.7 0.5  for experiment and production respectively!\n",
      "The ERR scores are  1.0 1.0  for experiment and production respectively!\n",
      "The NDCG scores @ k= 5  are:  0.33572413233 0.726228761795  for experiment and production respectively!!!\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "def check_performance(s):\n",
    "    prediction_e = s[0]\n",
    "    prediction_p = s[1]\n",
    "    print(prediction_e, prediction_p)\n",
    "    r = count_rel(prediction_e, prediction_p)\n",
    "    ap_e, ap_p = average_precision(prediction_e, r), average_precision(prediction_p, r)\n",
    "    print('The average prec. scores are ',ap_e,ap_p,' for experiment and production respectively!')\n",
    "    ERR_e, ERR_p = ERR(numeric_map, prediction_e), ERR(numeric_map, prediction_p)\n",
    "    print('The ERR scores are ',ERR_e,ERR_p,' for experiment and production respectively!')\n",
    "    opt_prediction = generate_opt(prediction_e,prediction_p)\n",
    "    ndcg_e = dcg_k(numeric_map, prediction_e, opt_prediction, k)\n",
    "    ndcg_p = dcg_k(numeric_map, prediction_p, opt_prediction, k)\n",
    "    print('The NDCG scores @ k=',k,' are: ',ndcg_e, ndcg_p,' for experiment and production respectively!!!')\n",
    "\n",
    "check_performance(combinations[20005])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
