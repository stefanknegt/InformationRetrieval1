{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretical part\n",
    "\n",
    "1a) P($m^{th}$ experiment gives significant result | m experiments lacking power to reject $H_0$)\n",
    "\n",
    "This probability of an experiment lacking the power to reject $H_0$ is ($1 -\\alpha  $). Assuming that consecutive experiments are  independent, the probability that the $m^{th}$ experiment gives significant result is: \n",
    "\n",
    "$(1-\\alpha)^{m-1} \\cdot \\alpha $ OR $ \\alpha$\n",
    "\n",
    "TODO: KIEZEN WELKE VERSIE VAN HET ANTWOORD HIERBOVEN WE INLEVEREN\n",
    "\n",
    "1b) P(at least one significant result | m experiments lacking power to reject $H_0$)\n",
    "\n",
    "This probability is given by: 1$-$ P(no significant results | m experiments lacking power to reject $H_0$) +\n",
    "This then equals:\n",
    "\n",
    "$\n",
    "1 - (1 - \\alpha  )^{m}  \n",
    "$\n",
    "\n",
    "2) \n",
    "\n",
    "Suppose we have the following two lists that we want to interleave.\n",
    "\n",
    "RankA\n",
    "1 2 4\n",
    "\n",
    "RankB\n",
    "2 3 4\n",
    "\n",
    "Suppose 1 and 2 are clicked on 48% of the times, and 3 is clicked on 4% of the times. This means RankA gives us 96% satisfaction and RankB gives us 52% satisfaction, so RankA is the best algorithm.\n",
    "\n",
    "The coin tosses and interleaved lists are given below\n",
    "\n",
    "$\\textbf{AA}$\n",
    "\n",
    "1A 2B 4A (A wins 50% of the time)\n",
    "\n",
    "\n",
    "$\\textbf{AB}$\n",
    "\n",
    "1A 2B 3B (A wins 48% of the time)\n",
    "\n",
    "\n",
    "$\\textbf{BA}$\n",
    "\n",
    "2B 1A 4A (A wins 50% of the time)\n",
    "\n",
    "\n",
    "$\\textbf{BB}$\n",
    "\n",
    "2B 1A 3B (A wins 48% of the time)\n",
    "\n",
    "This means that the team draft interleaving is unfair to the better algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental part\n",
    "\n",
    "1) Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the next section, we first create a list of all combinations of relevances. We use itertools.product which gives all possible combinations of a list in any order. Then we use permutation which gives all the combinations of experiment and production relevances. We use itertools.permutations to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "values = ['N','R','HR'] #possible values of a prediction\n",
    "\n",
    "relevances = [] #relevances contains all combinations of N/R/HR with length 5\n",
    "for r in itertools.product(values, repeat=5):\n",
    "    relevances.append(list(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinations = [] #combinations contains all pairs of relevances\n",
    "for p in itertools.permutations(relevances, 2):\n",
    "    combinations.append(list(p)) #we use this to get rid of the permutations object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'R']]]\n"
     ]
    }
   ],
   "source": [
    "print(combinations[:10]) #show the first 10 combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Step 2: Implement Evaluation Measures (10 points)\n",
    "\n",
    "In the next section we take two assumptions:\n",
    "\n",
    "1) Values for the prediction relevances are N=0, R=1, HR=2\n",
    "\n",
    "2) The amount of relevant predictions (overall) is assumed to be the total amount of relevant (R or HR) docs in the prediction set. So we assume there is no overlap between articles in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "#the first binary evaluation methods: average precision\n",
    "numeric_map = {'N':0, 'R':1, 'HR':2} #we use this numeric map to map N/R/HR to a numeric value.\n",
    "prediction = ['R','HR','N','R','N'] #this is a sample prediction to test functions\n",
    "\n",
    "def count_rel(prediction1,prediction2):\n",
    "    return sum(1 for i in prediction1 if i != 'N') + sum(1 for i in prediction2 if i != 'N')\n",
    "\n",
    "def average_precision(prediction, r):\n",
    "    ap = 0\n",
    "    relevant_preds = 0\n",
    "    for i in range(0,len(prediction)):\n",
    "        if prediction[i] != 'N':\n",
    "            relevant_preds += 1\n",
    "            ap += relevant_preds/(i+1)\n",
    "    return ap/r\n",
    "\n",
    "ap = average_precision(prediction, count_rel(prediction, prediction))\n",
    "print(ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement both multi-graded evaluation methods. \n",
    "\n",
    "The first is nDCG@k which requires a optimal prediction to normalize predictions. Here we will use the total amount of HR/R files to create an optimum prediction. Again we assume there is no overlap in predictions.\n",
    "\n",
    "The second one is ERR, this model does not need any assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53641800576\n"
     ]
    }
   ],
   "source": [
    "#nDCG@K\n",
    "import numpy as np #Numpy is amazing right?\n",
    "\n",
    "def generate_opt(prediction1, prediction2): #generate optimal sequences from two predictions\n",
    "    opt_pred = []\n",
    "    num_hr = sum(1 for i in prediction1 if i == 'HR') + sum(1 for i in prediction2 if i == 'HR')\n",
    "    num_r  = sum(1 for i in prediction1 if i == 'R') + sum(1 for i in prediction2 if i == 'R')\n",
    "    for i in range(min(num_hr,5)): #check if num_hr exceeds 5, fill with HR's\n",
    "        opt_pred.append('HR')\n",
    "    for i in range(min(5-num_hr,num_r)): #check if num_r exceeds the space left, will with R's\n",
    "        opt_pred.append('R')\n",
    "    for i in range(5-len(opt_pred)): #fill the rest with N\n",
    "        opt_pred.append('N')\n",
    "    return opt_pred\n",
    "\n",
    "def dcg_k(numeric_map, prediction, opt_pred, k):\n",
    "    dcg_opt = 0\n",
    "    dcg = 0\n",
    "    for i in range(0,k): #for the range until K, we sum both the optimum and prediction dcg\n",
    "        dcg_opt += (2**numeric_map[opt_pred[i]]-1)/np.log2(1+i+1)\n",
    "        dcg +=(2**numeric_map[prediction[i]]-1)/np.log2(1+i+1)\n",
    "    return dcg/dcg_opt #dcg is normalized compared to the optimum\n",
    "ndcg = dcg_k(numeric_map, prediction, generate_opt(prediction,prediction), 3)\n",
    "print(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.281982421875\n"
     ]
    }
   ],
   "source": [
    "#ERR\n",
    "def ERR(numeric_map, prediction):\n",
    "    err = 0\n",
    "    max_val = 2**max(list(numeric_map.values()))\n",
    "    thetas = [(2**numeric_map[p]-1)/max_val for p in prediction]\n",
    "    for i in range(0,len(prediction)):\n",
    "        prod_val = 1\n",
    "        for j in range(0,i):\n",
    "            prod_val *= (1-thetas[j])*thetas[i]\n",
    "        prod_val *= 1/(i+1)\n",
    "        err += prod_val\n",
    "    return err\n",
    "err = ERR(numeric_map, prediction)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Step 3: Calculate the 𝛥measure (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we calculate the delta measure for the three offline performance measures.\n",
    "import math\n",
    "def check_performance(combinations):\n",
    "    ap_counter = 0\n",
    "    ap_tie_counter = 0\n",
    "    ap_delta = []\n",
    "    err_counter = 0\n",
    "    err_tie_counter = 0\n",
    "    err_delta = []\n",
    "    ndcg_counter = 0\n",
    "    ndcg_tie_counter = 0\n",
    "    ndcg_delta = []\n",
    "    \n",
    "    total_count = len(combinations)\n",
    "    k = 5\n",
    "    for s in combinations:\n",
    "      \n",
    "        prediction_e = s[0]\n",
    "        prediction_p = s[1]\n",
    "\n",
    "        \n",
    "        r = count_rel(prediction_e, prediction_p)\n",
    "        ap_e, ap_p = average_precision(prediction_e, r), average_precision(prediction_p, r)\n",
    "        if ap_e > ap_p:\n",
    "            ap_counter += 1\n",
    "            ap_delta.append(ap_e-ap_p)\n",
    "        if ap_e == ap_p:\n",
    "            ap_tie_counter += 1\n",
    "            \n",
    "        ERR_e, ERR_p = ERR(numeric_map, prediction_e), ERR(numeric_map, prediction_p)\n",
    "        if ERR_e > ERR_p:\n",
    "            err_counter += 1\n",
    "            err_delta.append(ERR_e-ERR_p)\n",
    "        if ERR_e == ERR_p:\n",
    "            err_tie_counter += 1\n",
    "        opt_prediction = generate_opt(prediction_e,prediction_p)\n",
    "        ndcg_e = dcg_k(numeric_map, prediction_e, opt_prediction, k)\n",
    "        ndcg_p = dcg_k(numeric_map, prediction_p, opt_prediction, k)\n",
    "        if ndcg_e > ndcg_p:\n",
    "            ndcg_counter += 1\n",
    "            ndcg_delta.append(ndcg_e-ndcg_p)\n",
    "        if ndcg_e == ndcg_p:\n",
    "            ndcg_tie_counter += 1\n",
    "        \n",
    "    print(\"Percentage of cases that E outperforms P with average precision measure:\",round((ap_counter/total_count),2))\n",
    "    print(\"Percentage of cases that E outperforms P with ERR measure:\",round((err_counter/total_count),2))\n",
    "    print(\"Percentage of cases that E outperforms P with NDCG measure:\",round((ndcg_counter/total_count),2))\n",
    "    print(\"Percentage of cases that E ties with P with average precision measure:\",round((ap_tie_counter/total_count),2))\n",
    "    print(\"Percentage of cases that E ties with P with ERR measure:\",round((err_tie_counter/total_count),2))\n",
    "    print(\"Percentage of cases that E ties with P with NDCG measure:\",round((ndcg_tie_counter/total_count),2))\n",
    "    return ap_delta,err_delta,ndcg_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the delta measure show that for all three offline performance measures we see in 50% of the cases that the experiment ranking outperforms the production ranking. This is in line with expectations, since the combinations (rankings) are random and therefore we would expect that in a little less than 50% of the cases the experiment outperforms the production (since we also have ties its slightly less than 50%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Implement Interleaving (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_first is  False\n",
      "A_first is  True\n",
      "A_first is  True\n",
      "A_first is  False\n",
      "A_first is  True\n",
      "ranking_A is ['N', 'R', 'HR', 'HR', 'N']\n",
      "ranking_B is ['N', 'N', 'N', 'R', 'HR']\n",
      "I is ['N', 'N', 'R', 'N', 'HR']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tie'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def get_A_first(): #This is a function that determines is ranking A goes first or not\n",
    "    A = np.random.uniform() # Take a random uniform number between 0 and 1    \n",
    "    if A > 0.5: \n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def balanced_interleaving(s, cutoff = False):\n",
    "    \n",
    "    ranking_A = s[0]\n",
    "    ranking_B = s[1]\n",
    "    \n",
    "    # Initialize\n",
    "    I = []\n",
    "    k_a, k_b = 0,0\n",
    "        \n",
    "    A_first = get_A_first() #Find out if A or B goes first\n",
    "    \n",
    "    # We assume that rankA and rankB contain 10 unique documents\n",
    "    # That is why we can cast rankA and rankB to a dict\n",
    "    # This makes it easier to return a list of length 9, while adhering to the pseudo code from the slides\n",
    "    \n",
    "    rankA = {}\n",
    "    rankB = {}\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]\n",
    "    \n",
    "    for i in A:\n",
    "        rankA[i] = ranking_A[i]\n",
    "        \n",
    "    for j in B:\n",
    "        rankB[j] = ranking_B[j-5] \n",
    "        \n",
    "    # This code just follows the pseudo code from the slides\n",
    "    while k_a+1 <= len(ranking_A) and k_b+1 <= len(ranking_B):\n",
    "        if (k_a < k_b) or ((k_a == k_b) and A_first):\n",
    "            if A[k_a] not in I:\n",
    "                I.append(A[k_a])\n",
    "            k_a += 1\n",
    "            \n",
    "        else:\n",
    "            if B[k_b] not in I:\n",
    "                I.append(B[k_b])\n",
    "            k_b += 1\n",
    "             \n",
    "    # I is now filled with unique indices, we now have to convert these back to labels\n",
    "    I_ids = copy.copy(I)\n",
    "    for i in range(0,len(I)):\n",
    "        try:\n",
    "            I[i] = rankA[I[i]]\n",
    "        except:\n",
    "            I[i] = rankB[I[i]]\n",
    "    \n",
    "    if cutoff: #Cut-off to make the lenght of the list the same length as A and B\n",
    "        return I[:len(ranking_A)], I_ids[:len(ranking_A)], ranking_A, ranking_B\n",
    "    else:\n",
    "        return I, I_ids, ranking_A, ranking_B\n",
    "    \n",
    "def team_based_interleaving(s, cutoff = False):\n",
    "    \n",
    "    ranking_A = s[0]\n",
    "    ranking_B = s[1]\n",
    "    \n",
    "    # Initialize\n",
    "    I = []\n",
    "    k_a, k_b = 0,0\n",
    "    \n",
    "    # We assume that rankA and rankB contain 10 unique documents\n",
    "    # That is why we can cast rankA and rankB to a dict\n",
    "    # This makes it easier to return a list of length 9, while adhering to the pseudo code from the slides\n",
    "    \n",
    "    rankA = {}\n",
    "    rankB = {}\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]\n",
    "    \n",
    "    for i in A:\n",
    "        rankA[i] = ranking_A[i]\n",
    "        \n",
    "    for j in B:\n",
    "        rankB[j] = ranking_B[j-5] \n",
    "        \n",
    "    # This code just follows the pseudo code from the slides\n",
    "    teamA = 0\n",
    "    teamB = 0\n",
    "    \n",
    "    for i in range(0,len(ranking_A)):\n",
    "        A_first = get_A_first() # Flip a coin each time we go again\n",
    "        if A_first:\n",
    "            for idx in A: #Loop over A\n",
    "                if idx not in I: #Top result in A not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "            for idx in B: #Loop over B\n",
    "                if idx not in I: #Top result in B not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "        else:\n",
    "            for idx in B: #Loop over B\n",
    "                if idx not in I: #Top result in B not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "            for idx in A: #Loop over A\n",
    "                if idx not in I: #Top result in A not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "                \n",
    "    # I is now filled with unique indices, we now have to convert these back to labels\n",
    "    I_ids = copy.copy(I)\n",
    "    for i in range(0,len(I)):\n",
    "        try:\n",
    "            I[i] = rankA[I[i]]\n",
    "        except:\n",
    "            I[i] = rankB[I[i]]\n",
    "    \n",
    "    if cutoff: #Cut-off to make the lenght of the list the same length as A and B\n",
    "        return I[:len(ranking_A)], I_ids[:len(ranking_A)], ranking_A, ranking_B\n",
    "    else:\n",
    "        return I, I_ids, ranking_A, ranking_B\n",
    "\n",
    "def define_winner(clicks,I_ids):\n",
    "            \n",
    "    clicks_A = 0 # Number of clicks from result A\n",
    "    clicks_B = 0 # Number or clicks from result B\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]        \n",
    "            \n",
    "    for click in range(0,len(clicks)): # Loop over the clicks\n",
    "        if clicks[click]:\n",
    "            if I_ids[click] in A:\n",
    "                clicks_A += 1\n",
    "            elif I_ids[click] in B:\n",
    "                clicks_B += 1\n",
    "            \n",
    "    if clicks_A > clicks_B:\n",
    "        return \"A\"\n",
    "    elif clicks_B > clicks_A:\n",
    "        return \"B\"\n",
    "    else: \n",
    "        return \"Tie\"\n",
    "            \n",
    "test_set = combinations[12347]\n",
    "clicks = [True,True,False,False,False]\n",
    "\n",
    "# I, I_ids, rank_A, rank_B = balanced_interleaving(test_set, True)\n",
    "I, I_ids, rank_A, rank_B = team_based_interleaving(test_set, True)\n",
    "print ('I is',I)\n",
    "\n",
    "define_winner(clicks,I_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Step 5: Implement User Clicks Simulation (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example query, results and clicks:\n",
      "Query ID: 9\n",
      "Query results: [13, 70, 66, 94, 50, 104, 29, 21, 89, 85]\n",
      "Query clicks: [104, 21]\n",
      "We have  42652  answers/click sequences in total!\n"
     ]
    }
   ],
   "source": [
    "#read search query data\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def read_data():\n",
    "\n",
    "    answers = []\n",
    "    query_ids = []\n",
    "    clicks = []\n",
    "    click = []\n",
    "    last_type = 'C'\n",
    "\n",
    "    with open('YandexRelPredChallenge.txt') as f:\n",
    "        for line in f:\n",
    "            vals = re.split(r'\\t+', line.rstrip())\n",
    "            line_type = vals[2] #we look at the type of data line\n",
    "            if line_type == 'Q': #if type is query, we append the query.\n",
    "                if len(click) > 0: #we append clicks of last query before we go further\n",
    "                    clicks.append(click)\n",
    "                    click = []\n",
    "                answers.append(list(map(int, vals[5:])))\n",
    "                query_ids.append(int(vals[3]))\n",
    "            if last_type == 'Q' and line_type == 'Q': #If last type also was query there are no clicks  \n",
    "                clicks.append([])\n",
    "            elif line_type == 'C':\n",
    "                click.append(int(vals[3]))\n",
    "            last_type = vals[2]\n",
    "        clicks.append(click) #we shall not forget the last click sequence...\n",
    "    \n",
    "    return answers,query_ids,clicks\n",
    "\n",
    "answers,query_ids,clicks = read_data()\n",
    "\n",
    "print('Example query, results and clicks:')\n",
    "print(\"Query ID:\", query_ids[5])\n",
    "print(\"Query results:\", answers[5])\n",
    "print(\"Query clicks:\", clicks[5])\n",
    "print('We have ',len(clicks),' answers/click sequences in total!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Yandex dataset the probability that a (random) document gets clicked is: 0.13445559411047547\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Function to determine rho parameter of random click model given set of documents and clicks\n",
    "def rcm(documents,clicks):\n",
    "    unique_docs = []\n",
    "    unique_clicks = []\n",
    "    \n",
    "    assert(len(documents) == len(clicks))\n",
    "    \n",
    "    #First we find all unique documents to get the count\n",
    "    for d in documents:\n",
    "        for e in d:\n",
    "            unique_docs.append(e)\n",
    "\n",
    "    #number_of_docs = len(Counter(unique_docs).keys())\n",
    "    number_of_docs = len(unique_docs)\n",
    "    #Now we determine for the total number of cliks\n",
    "    for c in clicks:\n",
    "        for e in c:\n",
    "            unique_clicks.append(e)\n",
    "        \n",
    "    #number_of_clicks = len(Counter(unique_clicks).keys())\n",
    "    number_of_clicks = len(unique_clicks)\n",
    "    rho = number_of_clicks/number_of_docs\n",
    "    #print(number_of_clicks,number_of_docs)\n",
    "    return rho\n",
    "\n",
    "print(\"In the Yandex dataset the probability that a (random) document gets clicked is:\", rcm(answers,clicks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dynamic Bayesian network model\n",
    "#first we will look at sigma, as we can derive this by MLE directly. We \n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "query_clicks = {}\n",
    "query_sigma = {}\n",
    "for q in np.unique(query_ids)[:100]:\n",
    "    indices = [i for i, j in enumerate(query_ids) if j == q]\n",
    "    query_clicks[q] = list(itemgetter(*indices)(clicks))\n",
    "    last_clicks = []\n",
    "    for i in query_clicks[q]:\n",
    "        if isinstance(i, int):\n",
    "            last_clicks.append(query_clicks[q][-1])\n",
    "            break\n",
    "        elif len(i) > 0:\n",
    "            last_clicks.append(i[-1])\n",
    "        else:\n",
    "            last_clicks.append(0)\n",
    "    last_counter = Counter(last_clicks)\n",
    "    del last_counter[0]\n",
    "    try:\n",
    "        click_counter = Counter(list(itertools.chain(*query_clicks[q])))\n",
    "    except:\n",
    "        click_counter = Counter(query_clicks[q])\n",
    "    sigma_dict = dict(Counter({k:last_counter[k]/click_counter[k] for k in click_counter}))\n",
    "    query_sigma[q] = sigma_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the sigma's for the DBNM, however we cannot use these sigma's to evaluate e/p as we cannot link queries/documents. To be able to use the calculated sigma's, we will devide document query combinations in 3 parts: Not relevant, relevant and highly relevant (33% of query-document pairs each) and then we calculate the average sigma per group to use in the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R': 0.54446941082870659, 'N': 0.046705599807594686, 'HR': 0.98722901516920358}\n"
     ]
    }
   ],
   "source": [
    "sigmas = []\n",
    "for key in query_sigma:\n",
    "    [sigmas.append(i) for i in list(query_sigma[key].values())]\n",
    "sigmas = np.sort(sigmas)\n",
    "pairs = len(sigmas)\n",
    "\n",
    "sigma_dict = {'N': np.mean(sigmas[:int(pairs/3)]), 'R': np.mean(sigmas[int(pairs/3):-int(pairs/3)]), 'HR': np.mean(sigmas[-int(pairs/3):])}\n",
    "print(sigma_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplified model, we know which items are examined (either all items or all items until the last read item). We can devide the number of clicks by the number of examines to calculate alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R': 0.032043953614566648, 'N': 0.0, 'HR': 0.55290135325137568}\n"
     ]
    }
   ],
   "source": [
    "#We already have query-clicks so now we must develop query-examines. \n",
    "#If clicks is empty, we have examined every document. If we clicked something, we examined until the last doc.\n",
    "query_alphas = {}\n",
    "for q in np.unique(query_ids)[:100]:\n",
    "    examines = []\n",
    "    indices = [i for i, j in enumerate(query_ids) if j == q]\n",
    "    for i in indices:\n",
    "        answer = answers[i]\n",
    "        click = clicks[i] #we do not want to double track when someone clicks twice.\n",
    "        if len(click) == 0:\n",
    "            [examines.append(i) for i in answer]\n",
    "        else:\n",
    "            try:\n",
    "                last_click = max([answer.index(i) for i in click])\n",
    "                [examines.append(i) for i in answer[:last_click+1]]\n",
    "            except:\n",
    "                error = True\n",
    "    examines_counter = Counter(examines)\n",
    "    try:\n",
    "        click_counter = Counter(list(itertools.chain(*query_clicks[q])))\n",
    "    except:\n",
    "        click_counter = Counter(query_clicks[q])\n",
    "    #now we want to divide the clicks by the examines to get the attractive parameter\n",
    "    query_alphas[q] = dict(Counter({k:click_counter[k]/examines_counter[k] for k in examines_counter}))\n",
    "\n",
    "#now we do the same trick as before as we cannot link documents/queries. So we pick three categories.\n",
    "alphas = []\n",
    "for key in query_alphas:\n",
    "    [alphas.append(i) for i in list(query_alphas[key].values())]\n",
    "alphas = np.sort(alphas)\n",
    "pairs = len(alphas)\n",
    "\n",
    "alpha_dict = {'N': np.mean(alphas[:int(pairs/3)]), 'R': np.mean(alphas[int(pairs/3):-int(pairs/3)]), 'HR': np.mean(alphas[-int(pairs/3):])}\n",
    "print(alpha_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Simulate Interleaving Experiment (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run experiment n times with balanced interleaving and the random click model\n",
    "def run_interleaving_rcm(N):\n",
    "    \n",
    "    # Initialize random click model first\n",
    "    documents,_,clicks = read_data()\n",
    "    rho = rcm(documents,clicks)\n",
    "    \n",
    "    A_winner = 0\n",
    "    B_winner = 0\n",
    "    Tie = 0\n",
    "        \n",
    "    for combination in combinations:\n",
    "        I,I_ids,rank_A,rank_B = balanced_interleaving(combination,True)\n",
    "        # The cutoff bool determines the lenth of the interleaved list\n",
    "        # If set to false, the length is rank_A + rank_B - 1\n",
    "        # Is set to true, the length is of rank_A\n",
    "        \n",
    "        # We ignore order since it is a stochastic process anyway  \n",
    "        for n in range(0,N): #We decided to keep the interleaved list equal across the N trials.\n",
    "            clicks = []\n",
    "            for i in range(0,len(I)):\n",
    "                random_variable = np.random.uniform()\n",
    "\n",
    "                if random_variable < rho:\n",
    "                    clicks.append(True)\n",
    "                else:\n",
    "                    clicks.append(False)\n",
    "\n",
    "            winner = define_winner(clicks,I_ids)\n",
    "\n",
    "            if winner == \"A\":\n",
    "                A_winner += 1\n",
    "            elif winner == \"B\":\n",
    "                B_winner += 1\n",
    "            else:\n",
    "                Tie += 1\n",
    "            \n",
    "        \n",
    "    total = A_winner + B_winner + Tie    \n",
    "    print (\"Percentage of cases that E outperforms P:\",round(100*A_winner/total,2))\n",
    "    #print (\"P outperforms E\",100*B_winner/total, \"percent of the time\")\n",
    "    #print (\"It is a tie\",100*Tie/total, \"percent of the time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run experiment n times with balanced interleaving and the SDB click model\n",
    "def run_interleaving_SDBM(sigma_dict, alpha_dict,N):\n",
    "    A_winner = 0\n",
    "    B_winner = 0\n",
    "    Tie = 0\n",
    "        \n",
    "    for combination in combinations:\n",
    "        I,I_ids,rank_A,rank_B = balanced_interleaving(combination,True)\n",
    "        # The cutoff bool determines the lenth of the interleaved list\n",
    "        # If set to false, the length is rank_A + rank_B - 1\n",
    "        # Is set to true, the length is of rank_A\n",
    "        \n",
    "        for n in range(0,N):\n",
    "            clicks = []\n",
    "            satisfied = False\n",
    "\n",
    "            for i in range(0,len(I)):\n",
    "                relevance = I[i]\n",
    "                random_variable = np.random.uniform()\n",
    "\n",
    "                if random_variable < alpha_dict[relevance]: #here we check if the user is attracted\n",
    "                    if len(clicks) < len(I):\n",
    "                        clicks.append(True)\n",
    "                    random_variable_2 = np.random.uniform()\n",
    "                    if random_variable_2 < sigma_dict[relevance]: #here we look if the user was satisfied\n",
    "                        while len(clicks) < len(I):\n",
    "                            clicks.append(False) #if the user was satisfied, he does not click the rest of the list.\n",
    "                else:\n",
    "                    if len(clicks) < len(I):\n",
    "                        clicks.append(False)\n",
    "\n",
    "            winner = define_winner(clicks,I_ids)\n",
    "\n",
    "            if winner == \"A\":\n",
    "                A_winner += 1\n",
    "            elif winner == \"B\":\n",
    "                B_winner += 1\n",
    "            else:\n",
    "                Tie += 1\n",
    "\n",
    "        \n",
    "    total = A_winner + B_winner + Tie    \n",
    "    print (\"Percentage of caess that E outperforms P:\",round(100*A_winner/total,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Results and Analysis (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline evaluation: \n",
      "Percentage of cases that E outperforms P with average precision measure: 0.48\n",
      "Percentage of cases that E outperforms P with ERR measure: 0.5\n",
      "Percentage of cases that E outperforms P with NDCG measure: 0.5\n",
      "Percentage of cases that E ties with P with average precision measure: 0.05\n",
      "Percentage of cases that E ties with P with ERR measure: 0.0\n",
      "Percentage of cases that E ties with P with NDCG measure: 0.0\n",
      "\n",
      "Online evaluation with random click model and balanced interleaving: \n",
      "Percentage of cases that E outperforms P: 22.18\n",
      "\n",
      "Online evaluation with SDBM and balanced interleaving: \n",
      "Percentage of caess that E outperforms P: 32.77\n"
     ]
    }
   ],
   "source": [
    "print(\"Offline evaluation: \")\n",
    "check_performance(combinations)\n",
    "print(\"\\nOnline evaluation with random click model and balanced interleaving: \")\n",
    "#Random click model has a lot of ties bacause often there are 0 clicks and we can also have both 1 or 2 clicks.\n",
    "run_interleaving_rcm(5)\n",
    "print(\"\\nOnline evaluation with SDBM and balanced interleaving: \")\n",
    "run_interleaving_SDBM(sigma_dict, alpha_dict,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Conclusion offline and online evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Conclusion interleaving methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Conclusion click models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
