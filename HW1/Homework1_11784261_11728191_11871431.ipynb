{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical part\n",
    "\n",
    "<b>1a)</b>\n",
    "P($m^{th}$ experiment gives significant result | m experiments lacking power to reject $H_0$)\n",
    "\n",
    "This probability of an experiment lacking the power to reject $H_0$ is ($1 -\\alpha  $) so the probability that it gets rejected is $\\alpha$. Assuming that consecutive experiments are  independent, the probability that the $m^{th}$ experiment gives significant result is: \n",
    "\n",
    "$(1-\\alpha)^{m-1}\\alpha$\n",
    "\n",
    "<b>1b)</b> \n",
    "P(at least one significant result | m experiments lacking power to reject $H_0$)\n",
    "\n",
    "This probability is given by: 1$-$ P(no significant results | m experiments lacking power to reject $H_0$).\n",
    "This then equals:\n",
    "\n",
    "$\n",
    "1 - (1 - \\alpha  )^{m}  \n",
    "$\n",
    "\n",
    "<b>2)</b> \n",
    "\n",
    "Suppose we have the following two lists that we want to interleave based on team-draft interleaving.\n",
    "\n",
    "RankA\n",
    "1 2 4\n",
    "\n",
    "RankB\n",
    "2 3 4\n",
    "\n",
    "Suppose 1 and 2 are clicked on 48% of the times, and 3 is clicked on 4% of the times. This means that the total clicks odds for RankA is 96% and 52% for RankB. Therefore RankA is the best algorithm.\n",
    "\n",
    "The coin tosses and interleaved lists are given below\n",
    "\n",
    "$\\textbf{AA}$\n",
    "\n",
    "1A 2B 4A (A wins 50% of the time)\n",
    "\n",
    "\n",
    "$\\textbf{AB}$\n",
    "\n",
    "1A 2B 3B (A wins 48% of the time)\n",
    "\n",
    "\n",
    "$\\textbf{BA}$\n",
    "\n",
    "2B 1A 4A (A wins 50% of the time)\n",
    "\n",
    "\n",
    "$\\textbf{BB}$\n",
    "\n",
    "2B 1A 3B (A wins 48% of the time)\n",
    "\n",
    "As B wins more often on average, this means that the team draft interleaving is unfair to the better algorithm in this specific case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental part\n",
    "\n",
    "## Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the next section, we first create a list of all combinations of relevances. We use itertools.product which gives all possible combinations of a list in any order. Then we use permutation which gives all the combinations of experiment and production relevances. We use itertools.permutations to do this. This means there are no duplicate results in combinations (same E-relevances as P-relevances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'R'], ['N', 'N', 'N', 'N', 'HR'], ['N', 'N', 'N', 'R', 'N'], ['N', 'N', 'N', 'R', 'R'], ['N', 'N', 'N', 'R', 'HR'], ['N', 'N', 'N', 'HR', 'N'], ['N', 'N', 'N', 'HR', 'R'], ['N', 'N', 'N', 'HR', 'HR'], ['N', 'N', 'R', 'N', 'N']]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "values = ['N','R','HR'] #possible values of a prediction\n",
    "\n",
    "relevances = [] #relevances contains all combinations of N/R/HR with length 5\n",
    "for r in itertools.product(values, repeat=5):\n",
    "    relevances.append(list(r))\n",
    "print(relevances[:10]) #print first 10 relevance combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinations = [] #combinations contains all pairs of relevances\n",
    "for p in itertools.permutations(relevances, 2):\n",
    "    combinations.append(list(p)) #we use this to get rid of the permutations object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'R']]]\n"
     ]
    }
   ],
   "source": [
    "print(combinations[:10]) #show the first 10 permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement Evaluation Measures (10 points)\n",
    "\n",
    "In the next section we take two assumptions:\n",
    "\n",
    "1) Value mapping for the prediction relevances are N=0, R=1, HR=2.\n",
    "\n",
    "2) The amount of relevant predictions (overall) is assumed to be the total amount of relevant (R or HR) docs in the prediction set. So we assume there is no overlap between articles in the predictions of E/P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "#the first binary evaluation methods: average precision\n",
    "numeric_map = {'N':0, 'R':1, 'HR':2} #we use this numeric map to map N/R/HR to a numeric value.\n",
    "prediction = ['R','HR','N','R','N'] #this is a sample prediction to test functions\n",
    "\n",
    "def count_rel(prediction1,prediction2): #this function counts the total amount of relevant docs in both results.\n",
    "    return sum(1 for i in prediction1 if i != 'N') + sum(1 for i in prediction2 if i != 'N')\n",
    "\n",
    "def average_precision(prediction, r):\n",
    "    ap = 0\n",
    "    relevant_preds = 0\n",
    "    for i in range(0,len(prediction)): #for every element of the prediction\n",
    "        if prediction[i] != 'N': #if it is a relevant one\n",
    "            relevant_preds += 1 #add one to the cumulative relevant documents\n",
    "            ap += relevant_preds/(i+1) #calculate precision cumulative\n",
    "    return ap/r #take the average.\n",
    "\n",
    "ap = average_precision(prediction, count_rel(prediction, prediction))\n",
    "print(ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement both multi-graded evaluation methods. \n",
    "\n",
    "The first is nDCG@k which requires a optimal prediction to normalize predictions. Here we will use the total amount of HR/R files to create an optimum prediction. Again we assume there is no overlap in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53641800576\n"
     ]
    }
   ],
   "source": [
    "#nDCG@K\n",
    "import numpy as np #numpy is amazing right?\n",
    "\n",
    "def generate_opt(prediction1, prediction2): #generate optimal sequences from two predictions\n",
    "    opt_pred = []\n",
    "    num_hr = sum(1 for i in prediction1 if i == 'HR') + sum(1 for i in prediction2 if i == 'HR')\n",
    "    num_r  = sum(1 for i in prediction1 if i == 'R') + sum(1 for i in prediction2 if i == 'R')\n",
    "    for i in range(min(num_hr,5)): #check if num_hr exceeds 5, fill with HR's\n",
    "        opt_pred.append('HR')\n",
    "    for i in range(min(5-num_hr,num_r)): #check if num_r exceeds the space left, will with R's\n",
    "        opt_pred.append('R')\n",
    "    for i in range(5-len(opt_pred)): #fill the rest with N\n",
    "        opt_pred.append('N')\n",
    "    return opt_pred\n",
    "\n",
    "def dcg_k(numeric_map, prediction, opt_pred, k):\n",
    "    dcg_opt = 0\n",
    "    dcg = 0\n",
    "    for i in range(0,k): #for the range until K, we sum both the optimum and prediction dcg\n",
    "        dcg_opt += (2**numeric_map[opt_pred[i]]-1)/np.log2(1+i+1)\n",
    "        dcg +=(2**numeric_map[prediction[i]]-1)/np.log2(1+i+1)\n",
    "    return dcg/dcg_opt #dcg is normalized compared to the optimum\n",
    "ndcg = dcg_k(numeric_map, prediction, generate_opt(prediction,prediction), 3) #K=3 is used in this example\n",
    "print(ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second one is ERR, this model does not need any assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.281982421875\n"
     ]
    }
   ],
   "source": [
    "#ERR\n",
    "def ERR(numeric_map, prediction):\n",
    "    err = 0\n",
    "    max_val = 2**max(list(numeric_map.values())) #we calculate the maximum possible value from numeric mapping.\n",
    "    thetas = [(2**numeric_map[p]-1)/max_val for p in prediction] #for every prediction, we estimate theta value\n",
    "    for i in range(0,len(prediction)):\n",
    "        pred_val = 1 #we start with 1 as we want to do cumulative multiplication\n",
    "        for j in range(0,i): #loop back over earlier predictions\n",
    "            pred_val *= (1-thetas[j])*thetas[i] \n",
    "        pred_val *= 1/(i+1)\n",
    "        err += pred_val #add all values of individual predictions\n",
    "    return err\n",
    "err = ERR(numeric_map, prediction)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate the ùõ•measure (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we calculate the delta measure for the three offline performance measures. We know that this function returns a lot of information,\n",
    "#however we need all this for different statistical analyses.\n",
    "import math\n",
    "def check_performance(combinations,printLog=True):\n",
    "    #Here we initialize the counters for statistics\n",
    "    ap_counter = 0\n",
    "    ap_tie_counter = 0\n",
    "    ap_delta = []\n",
    "    total_ap_e = []\n",
    "    total_ap_p = []\n",
    "    err_counter = 0\n",
    "    err_tie_counter = 0\n",
    "    err_delta = []\n",
    "    total_err_e = []\n",
    "    total_err_p = []\n",
    "    ndcg_counter = 0\n",
    "    ndcg_tie_counter = 0\n",
    "    ndcg_delta = []\n",
    "    total_ndcg_e = []\n",
    "    total_ndcg_p = []\n",
    "    return_list = []\n",
    "    all_delta_ap = []\n",
    "    all_delta_err = []\n",
    "    all_delta_ndcg = []\n",
    "    total_count = len(combinations) #total amount of combinations we want to loop over\n",
    "    k = 5 #K is used for nDCG@K\n",
    "    for s in combinations:\n",
    "        prediction_e = s[0] #split the predictions\n",
    "        prediction_p = s[1]\n",
    "\n",
    "        r = count_rel(prediction_e, prediction_p) #count relevant docs for both predictions\n",
    "        ap_e, ap_p = average_precision(prediction_e, r), average_precision(prediction_p, r) #calculate ap\n",
    "        if ap_e > ap_p: #we only want to look at cases where e>p\n",
    "            ap_counter += 1\n",
    "            ap_delta.append(ap_e-ap_p) #we append the difference between e and p.\n",
    "\n",
    "        all_delta_ap.append(ap_e-ap_p)\n",
    "        if ap_e == ap_p: #we also want to track ties\n",
    "            ap_tie_counter += 1\n",
    "        total_ap_e.append(ap_e)\n",
    "        total_ap_p.append(ap_p)\n",
    "\n",
    "\n",
    "        ERR_e, ERR_p = ERR(numeric_map, prediction_e), ERR(numeric_map, prediction_p) #calculate ERR scores\n",
    "        if ERR_e > ERR_p: #same as before\n",
    "            err_counter += 1\n",
    "            err_delta.append(ERR_e-ERR_p)\n",
    "        all_delta_err.append(ERR_e-ERR_p)\n",
    "        if ERR_e == ERR_p:\n",
    "            err_tie_counter += 1\n",
    "        total_err_e.append(ERR_e)\n",
    "        total_err_p.append(ERR_p)\n",
    "\n",
    "\n",
    "        opt_prediction = generate_opt(prediction_e,prediction_p) #first we need the optimal prediction for normalization\n",
    "        ndcg_e = dcg_k(numeric_map, prediction_e, opt_prediction, k) #calculate scoring for e\n",
    "        ndcg_p = dcg_k(numeric_map, prediction_p, opt_prediction, k) #calculate scoring for p\n",
    "        if ndcg_e > ndcg_p: #same as before\n",
    "            ndcg_counter += 1\n",
    "            ndcg_delta.append(ndcg_e-ndcg_p)\n",
    "        all_delta_ndcg.append(ndcg_e-ndcg_p)\n",
    "        if ndcg_e == ndcg_p:\n",
    "            ndcg_tie_counter += 1\n",
    "        total_ndcg_e.append(ndcg_e)\n",
    "        total_ndcg_p.append(ndcg_p)\n",
    "\n",
    "\n",
    "        return_list.append([(ap_e-ap_p),(ERR_e-ERR_p),(ndcg_e-ndcg_p)])\n",
    "\n",
    "    if printLog == True:\n",
    "        print(\"Percentage of cases that E outperforms P with average precision measure:\",round((ap_counter/total_count),2), \" and percentage of ties is: \",round((ap_tie_counter/total_count),2))\n",
    "        print(\"Percentage of cases that E outperforms P with ERR measure:\",round((err_counter/total_count),2),\"and percentage of ties is: \",round((err_tie_counter/total_count),2))\n",
    "        print(\"Percentage of cases that E outperforms P with NDCG measure:\",round((ndcg_counter/total_count),2),\"and percentage of ties is: \",round((ndcg_tie_counter/total_count),2))\n",
    "\n",
    "    return ap_delta,err_delta,ndcg_delta,total_ap_e,total_ap_p,total_err_e,total_err_p,total_ndcg_e,total_ndcg_p,return_list,all_delta_ap,all_delta_err,all_delta_ndcg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Interleaving (15 points)\n",
    "We implemented both team based and balanced interleaving as this allows us to get a better insight in bias of both techniques. This will be discussed in the conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tie'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def get_A_first(): #This is a function that determines is ranking A goes first or not\n",
    "    A = np.random.uniform() # Take a random uniform number between 0 and 1    \n",
    "    if A > 0.5: \n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def balanced_interleaving(s, cutoff = False):\n",
    "    \n",
    "    ranking_A = s[0]\n",
    "    ranking_B = s[1]\n",
    "    \n",
    "    # Initialize\n",
    "    I = []\n",
    "    k_a, k_b = 0,0\n",
    "        \n",
    "    A_first = get_A_first() #Find out if A or B goes first\n",
    "    \n",
    "    # We assume that rankA and rankB contain 10 unique documents\n",
    "    # That is why we can cast rankA and rankB to a dict\n",
    "    # This makes it easier to return a list of length 9, while adhering to the pseudo code from the slides\n",
    "    \n",
    "    rankA = {}\n",
    "    rankB = {}\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]\n",
    "    \n",
    "    for i in A:\n",
    "        rankA[i] = ranking_A[i]\n",
    "        \n",
    "    for j in B:\n",
    "        rankB[j] = ranking_B[j-5] \n",
    "        \n",
    "    # This code just follows the pseudo code from the slides\n",
    "    while k_a+1 <= len(ranking_A) and k_b+1 <= len(ranking_B):\n",
    "        if (k_a < k_b) or ((k_a == k_b) and A_first):\n",
    "            if A[k_a] not in I:\n",
    "                I.append(A[k_a])\n",
    "            k_a += 1\n",
    "            \n",
    "        else:\n",
    "            if B[k_b] not in I:\n",
    "                I.append(B[k_b])\n",
    "            k_b += 1\n",
    "             \n",
    "    # I is now filled with unique indices, we now have to convert these back to labels\n",
    "    I_ids = copy.copy(I)\n",
    "    for i in range(0,len(I)):\n",
    "        try:\n",
    "            I[i] = rankA[I[i]]\n",
    "        except:\n",
    "            I[i] = rankB[I[i]]\n",
    "    \n",
    "    if cutoff: #Cut-off to make the lenght of the list the same length as A and B\n",
    "        return I[:len(ranking_A)], I_ids[:len(ranking_A)], ranking_A, ranking_B\n",
    "    else:\n",
    "        return I, I_ids, ranking_A, ranking_B\n",
    "    \n",
    "def team_based_interleaving(s, cutoff = False):\n",
    "    \n",
    "    ranking_A = s[0]\n",
    "    ranking_B = s[1]\n",
    "    \n",
    "    # Initialize\n",
    "    I = []\n",
    "    k_a, k_b = 0,0\n",
    "    \n",
    "    # We assume that rankA and rankB contain 10 unique documents\n",
    "    # That is why we can cast rankA and rankB to a dict\n",
    "    # This makes it easier to return a list of length 9, while adhering to the pseudo code from the slides\n",
    "    \n",
    "    rankA = {}\n",
    "    rankB = {}\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]\n",
    "    \n",
    "    for i in A:\n",
    "        rankA[i] = ranking_A[i]\n",
    "        \n",
    "    for j in B:\n",
    "        rankB[j] = ranking_B[j-5] \n",
    "        \n",
    "    # This code just follows the pseudo code from the slides\n",
    "    teamA = 0\n",
    "    teamB = 0\n",
    "    \n",
    "    for i in range(0,len(ranking_A)):\n",
    "        A_first = get_A_first() # Flip a coin each time we go again\n",
    "        if A_first:\n",
    "            for idx in A: #Loop over A\n",
    "                if idx not in I: #Top result in A not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "            for idx in B: #Loop over B\n",
    "                if idx not in I: #Top result in B not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "        else:\n",
    "            for idx in B: #Loop over B\n",
    "                if idx not in I: #Top result in B not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "            for idx in A: #Loop over A\n",
    "                if idx not in I: #Top result in A not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "                \n",
    "    # I is now filled with unique indices, we now have to convert these back to labels\n",
    "    I_ids = copy.copy(I)\n",
    "    for i in range(0,len(I)):\n",
    "        try:\n",
    "            I[i] = rankA[I[i]]\n",
    "        except:\n",
    "            I[i] = rankB[I[i]]\n",
    "    \n",
    "    if cutoff: #Cut-off to make the lenght of the list the same length as A and B\n",
    "        return I[:len(ranking_A)], I_ids[:len(ranking_A)], ranking_A, ranking_B\n",
    "    else:\n",
    "        return I, I_ids, ranking_A, ranking_B\n",
    "\n",
    "def define_winner(clicks,I_ids): # The function that defines the winner from an interleaved list. \n",
    "    \n",
    "    if clicks.count(False) == 5: # we chose to seperate the queries with no clicks, while they are actually a tie\n",
    "        # this makes it easier for discussion in the conclusion part\n",
    "        return \"noClick\"\n",
    "            \n",
    "    clicks_A = 0 # Number of clicks from result A\n",
    "    clicks_B = 0 # Number or clicks from result B\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]        \n",
    "            \n",
    "    for click in range(0,len(clicks)): # Loop over the clicks\n",
    "        if clicks[click]:\n",
    "            if I_ids[click] in A:\n",
    "                clicks_A += 1\n",
    "            elif I_ids[click] in B:\n",
    "                clicks_B += 1\n",
    "            \n",
    "    if clicks_A > clicks_B:\n",
    "        return \"A\"\n",
    "    elif clicks_B > clicks_A:\n",
    "        return \"B\"\n",
    "    else: \n",
    "        return \"Tie\"\n",
    "            \n",
    "test_set = combinations[12347]\n",
    "test_clicks = [True,True,False,False,False]\n",
    "\n",
    "# I, I_ids, rank_A, rank_B = balanced_interleaving(test_set, True)\n",
    "I, I_ids, rank_A, rank_B = team_based_interleaving(test_set, True)\n",
    "\n",
    "define_winner(test_clicks,I_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 5: Implement User Clicks Simulation (15 points)\n",
    " First we will need to import the YandexRel dataset. Then we implement the random click model whereafter we implement a simplified Dynamic Baseysian Network Model.\n",
    " \n",
    "In the query data, we assume only one result page is observed and that each result page is a \"new query\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example query, results and clicks:\n",
      "Query ID: 9\n",
      "Query results: [13, 70, 66, 94, 50, 104, 29, 21, 89, 85]\n",
      "Query clicks: [104, 21]\n",
      "We have  42652  answers/click sequences in total!\n"
     ]
    }
   ],
   "source": [
    "#read search query data\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def read_data():\n",
    "\n",
    "    answers = []\n",
    "    query_ids = []\n",
    "    clicks = []\n",
    "    click = []\n",
    "    last_type = 'C'\n",
    "\n",
    "    with open('YandexRelPredChallenge.txt') as f:\n",
    "        for line in f:\n",
    "            vals = re.split(r'\\t+', line.rstrip())\n",
    "            line_type = vals[2] #we look at the type of data line\n",
    "            if line_type == 'Q': #if type is query, we append the query.\n",
    "                if len(click) > 0: #we append clicks of last query before we go further\n",
    "                    clicks.append(click)\n",
    "                    click = []\n",
    "                answers.append(list(map(int, vals[5:])))\n",
    "                query_ids.append(int(vals[3]))\n",
    "            if last_type == 'Q' and line_type == 'Q': #If last type also was query there are no clicks  \n",
    "                clicks.append([])\n",
    "            elif line_type == 'C':\n",
    "                click.append(int(vals[3]))\n",
    "            last_type = vals[2]\n",
    "        clicks.append(click) #we shall not forget the last click sequence...\n",
    "    \n",
    "    return answers,query_ids,clicks\n",
    "\n",
    "answers,query_ids,clicks = read_data()\n",
    "\n",
    "print('Example query, results and clicks:')\n",
    "print(\"Query ID:\", query_ids[5])\n",
    "print(\"Query results:\", answers[5])\n",
    "print(\"Query clicks:\", clicks[5])\n",
    "print('We have ',len(clicks),' answers/click sequences in total!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the random click model, the number of clicks need to be divided by the number of docs shown to a user. The function returns the parameter rho which indicates the click probability per result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Yandex dataset the probability that a (random) document gets clicked is: 0.13445559411047547\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Function to determine rho parameter of random click model given set of documents and clicks\n",
    "def rcm(documents,clicks):\n",
    "    tot_docs = []\n",
    "    tot_clicks = []\n",
    "    \n",
    "    assert(len(documents) == len(clicks))\n",
    "    \n",
    "    #First we find all  documents to get the count\n",
    "    for d in documents:\n",
    "        for e in d:\n",
    "            tot_docs.append(e)\n",
    "    number_of_docs = len(tot_docs)\n",
    "   \n",
    "    #Now we determine for the total number of cliks\n",
    "    for c in clicks:\n",
    "        for e in c:\n",
    "            tot_clicks.append(e)\n",
    "    number_of_clicks = len(tot_clicks)\n",
    "    \n",
    "    rho = number_of_clicks/number_of_docs\n",
    "\n",
    "    return rho\n",
    "\n",
    "print(\"In the Yandex dataset the probability that a (random) document gets clicked is:\", rcm(answers,clicks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement the Dynamic Bayesian Network Model which is actually the so-called simplified DBN model (SDBN), since we make the assumption that $\\gamma = 1$. We do this, because by doing so we can easily estimate the model parameters and it has been shown that the predictive power of this model is similar to the not simplified version.\n",
    "\n",
    "First we calculate sigma, which is determined for every query document pair. The sigma can be calculated by dividing the amount of times a document was the last document clicked (only way which a document was satisfying when $\\gamma=1$) divided by the amount of times the document was clicked for a specific query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dynamic Bayesian network model\n",
    "#first we will look at sigma, as we can derive this by MLE directly as the simplification of gamma=1 is made.\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "query_clicks = {} #here we save a dict of clicks per query.\n",
    "query_sigma = {} #here we save a dict of sigma's per document per query.\n",
    "for q in np.unique(query_ids): #look at all distinct queries\n",
    "    indices = [i for i, j in enumerate(query_ids) if j == q]\n",
    "    query_clicks[q] = list(itemgetter(*indices)(clicks)) #look at clicks at those indices.\n",
    "    last_clicks = [] #track last clicks\n",
    "    for i in query_clicks[q]:\n",
    "        if isinstance(i, int): #these rules are because sometimes the query_clicks is not a list of lists.\n",
    "            last_clicks.append(query_clicks[q][-1])\n",
    "            break\n",
    "        elif len(i) > 0:\n",
    "            last_clicks.append(i[-1])\n",
    "        else:\n",
    "            last_clicks.append(0)\n",
    "    last_counter = Counter(last_clicks)\n",
    "    del last_counter[0] #get rid of the zeros as they were dummies\n",
    "    try:\n",
    "        click_counter = Counter(list(itertools.chain(*query_clicks[q]))) #in case of list of lists\n",
    "    except:\n",
    "        click_counter = Counter(query_clicks[q]) #in case of a single list.\n",
    "    sigma_dict = dict(Counter({k:last_counter[k]/click_counter[k] for k in click_counter})) #divide last_counter by click_counter to get sigma\n",
    "    query_sigma[q] = sigma_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the sigma's for the SDBN, however we cannot use these sigma's to evaluate e/p as we cannot link queries/documents. To be able to use the calculated sigma's, we will devide document query combinations in 3 parts: Not relevant, relevant and highly relevant (33% of query-document pairs each) and then we calculate the average sigma per group to use in the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 0.0, 'R': 0.45495814707878407, 'HR': 1.0}\n"
     ]
    }
   ],
   "source": [
    "sigmas = []\n",
    "for key in query_sigma:\n",
    "    [sigmas.append(i) for i in list(query_sigma[key].values())] #append all sigma values into a list\n",
    "sigmas = np.sort(sigmas) #sort the list\n",
    "pairs = len(sigmas) #look at length of the list\n",
    "\n",
    "sigma_dict = {'N': np.mean(sigmas[:int(pairs/3)]), 'R': np.mean(sigmas[int(pairs/3):-int(pairs/3)]), 'HR': np.mean(sigmas[-int(pairs/3):])}\n",
    "print(sigma_dict) #show the results of the sigma dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to calculate alpha. In the simplified model, we know which items are examined (either all items or all items until the last read item). We can devide the number of clicks by the number of examines to calculate alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We already have query-clicks so now we must develop query-examines. \n",
    "#If clicks is empty, we have examined every document. If we clicked something, we examined until the last doc.\n",
    "query_alphas = {}\n",
    "for q in np.unique(query_ids): #we look at distinct queries again\n",
    "    examines = []\n",
    "    indices = [i for i, j in enumerate(query_ids) if j == q]\n",
    "    for i in indices:\n",
    "        answer = answers[i] #look at the answers\n",
    "        click = clicks[i] #we do not want to double track when someone clicks twice.\n",
    "        if len(click) == 0:\n",
    "            [examines.append(i) for i in answer] #user does not click, all results are appended to examines\n",
    "        else:\n",
    "            try:\n",
    "                last_click = max([answer.index(i) for i in click]) #we look at the click with the max index\n",
    "                [examines.append(i) for i in answer[:last_click+1]] #we append all documents which are before the last click\n",
    "            except:\n",
    "                error = True\n",
    "    examines_counter = Counter(examines) #we make a counter of the examines\n",
    "    try:\n",
    "        click_counter = Counter(list(itertools.chain(*query_clicks[q])))\n",
    "    except:\n",
    "        click_counter = Counter(query_clicks[q])\n",
    "    #now we want to divide the clicks by the examines to get the attractive parameter\n",
    "    query_alphas[q] = dict(Counter({k:click_counter[k]/examines_counter[k] for k in examines_counter}))\n",
    "\n",
    "#now we do the same trick as before as we cannot link documents/queries. So we pick three categories.\n",
    "alphas = []\n",
    "for key in query_alphas:\n",
    "    [alphas.append(i) for i in list(query_alphas[key].values())]\n",
    "alphas = np.sort(alphas)\n",
    "pairs = len(alphas)\n",
    "\n",
    "alpha_dict = {'N': np.mean(alphas[:int(pairs/3)]), 'R': np.mean(alphas[int(pairs/3):-int(pairs/3)]), 'HR': np.mean(alphas[-int(pairs/3):])}\n",
    "print(alpha_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Simulate Interleaving Experiment (10 points)\n",
    "Now we want to simulate based on both click models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run experiment n times with balanced interleaving and the random click model\n",
    "def run_interleaving_rcm(N,interleaving):\n",
    "    \n",
    "    # Initialize random click model first\n",
    "    documents,_,clicks = read_data()\n",
    "    rho = rcm(documents,clicks)\n",
    "    \n",
    "    A_winner = 0\n",
    "    B_winner = 0\n",
    "    noClick = 0\n",
    "    Tie = 0\n",
    "    win_list = []\n",
    "    one_clicks = 0\n",
    "    \n",
    "    for combination in combinations:\n",
    "        combinationA_winner = 0\n",
    "        combinationB_winner = 0\n",
    "        if(interleaving == 1):\n",
    "            I,I_ids,rank_A,rank_B = balanced_interleaving(combination,True)\n",
    "        elif(interleaving == 2):\n",
    "            I,I_ids,rank_A,rank_B = team_based_interleaving(combination,True)\n",
    "        else:\n",
    "            print(\"Please use 1 for balanced interleaving and 2 for team based interleaving.\")\n",
    "        # The cutoff bool determines the lenth of the interleaved list\n",
    "        # If set to false, the length is rank_A + rank_B - 1\n",
    "        # Is set to true, the length is of rank_A\n",
    "        \n",
    "        # We ignore order since it is a stochastic process anyway  \n",
    "        for n in range(0,N): #We decided to keep the interleaved list equal across the N trials.\n",
    "            clicks = []\n",
    "            for i in range(0,len(I)):\n",
    "                random_variable = np.random.uniform()\n",
    "\n",
    "                if random_variable < rho: #we randomly determine a click with odds rho\n",
    "                    clicks.append(True)\n",
    "                else:\n",
    "                    clicks.append(False)\n",
    "            \n",
    "            if clicks.count(True) == 1:\n",
    "                one_clicks += 1\n",
    "\n",
    "            winner = define_winner(clicks,I_ids) #define winner tells who is the winner based on clicks.\n",
    "            \n",
    "            if winner == \"A\":\n",
    "                A_winner += 1\n",
    "                combinationA_winner += 1\n",
    "            elif winner == \"B\":\n",
    "                B_winner += 1\n",
    "                combinationB_winner += 1\n",
    "            elif winner == \"noClick\":\n",
    "                noClick += 1\n",
    "            else:\n",
    "                Tie += 1\n",
    "\n",
    "        win_list.append([combinationA_winner/N,combinationB_winner/N,(N-(combinationA_winner+combinationB_winner))/N]) \n",
    "    \n",
    "    total = A_winner + B_winner + Tie + noClick\n",
    "    print (\"Percentage of cases that E outperforms P:\",round(100*A_winner/total,2),\"and percentage of ties is: \",round(100*Tie/total,2))\n",
    "    print (round(100*noClick / total,2),\"% of queries does not have a click\")\n",
    "    print (round(100*one_clicks / total,2),\"% of queries has one click\")\n",
    "    #print (\"P outperforms E\",100*B_winner/total, \"percent of the time\")\n",
    "    #print (\"It is a tie\",100*Tie/total, \"percent of the time\")\n",
    "    return win_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run experiment n times with balanced interleaving and the SDB click model\n",
    "def run_interleaving_SDBM(sigma_dict, alpha_dict,N,interleaving):\n",
    "    A_winner = 0\n",
    "    B_winner = 0\n",
    "    noClick = 0\n",
    "    Tie = 0\n",
    "    win_list = []\n",
    "    one_clicks = 0\n",
    "    \n",
    "    for combination in combinations:\n",
    "        combinationA_winner = 0\n",
    "        combinationB_winner = 0\n",
    "        if(interleaving == 1):\n",
    "            I,I_ids,rank_A,rank_B = balanced_interleaving(combination,True)\n",
    "        elif(interleaving == 2):\n",
    "            I,I_ids,rank_A,rank_B = team_based_interleaving(combination,True)\n",
    "        else:\n",
    "            print(\"Please use 1 for balanced interleaving and 2 for team based interleaving.\")\n",
    "        # The cutoff bool determines the lenth of the interleaved list\n",
    "        # If set to false, the length is rank_A + rank_B - 1\n",
    "        # Is set to true, the length is of rank_A\n",
    "        \n",
    "        for n in range(0,N):\n",
    "            clicks = []\n",
    "            satisfied = False\n",
    "\n",
    "            for i in range(0,len(I)):\n",
    "                relevance = I[i]\n",
    "                random_variable = np.random.uniform()\n",
    "\n",
    "                if random_variable < alpha_dict[relevance]: #here we check if the user is attracted\n",
    "                    if len(clicks) < len(I): #we cannot click more documents than results in list.\n",
    "                        clicks.append(True)\n",
    "                    random_variable_2 = np.random.uniform()\n",
    "                    if random_variable_2 < sigma_dict[relevance]: #here we look if the user was satisfied\n",
    "                        while len(clicks) < len(I):\n",
    "                            clicks.append(False) #if the user was satisfied, he does not click the rest of the list.\n",
    "                else:\n",
    "                    if len(clicks) < len(I):\n",
    "                        clicks.append(False)\n",
    "\n",
    "            if clicks.count(True) == 1:\n",
    "                one_clicks += 1\n",
    "\n",
    "            winner = define_winner(clicks,I_ids)\n",
    "\n",
    "            if winner == \"A\":\n",
    "                A_winner += 1\n",
    "                combinationA_winner += 1\n",
    "            elif winner == \"B\":\n",
    "                B_winner += 1\n",
    "                combinationB_winner += 1\n",
    "            elif winner == \"noClick\":\n",
    "                noClick += 1\n",
    "            else:\n",
    "                Tie += 1\n",
    "\n",
    "        win_list.append([combinationA_winner/N,combinationB_winner/N,(N-(combinationA_winner+combinationB_winner))/N]) \n",
    "        \n",
    "    total = A_winner + B_winner + Tie + noClick    \n",
    "    print (\"Percentage of cases that E outperforms P:\",round(100*A_winner/total,2),\"and percentage of ties is: \",round(100*Tie/total,2))\n",
    "    print (round(100*noClick / total,2),\"% of queries does not have a click\")\n",
    "    print (round(100*one_clicks / total,2),\"% of queries has one click\")\n",
    "    return win_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Results and Analysis (30 points)\n",
    "First we will develop top line results of the combinations. This gives a brief insight about the percentage of wins/loses/ties for the individual scoring methods.\n",
    "\n",
    "Then we will look at the agreement of individual scoring metrics, so we can develop a better understanding whether they agree on E/P being a better model. Furthermore, we will look at statistical significance of the results. Also we will briefly talk about the difference between interleaving methods and click models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion part 1) High over results<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 10 #number of simulations per combination\n",
    "\n",
    "print(\"Offline evaluation: \")\n",
    "ap,err,ndcg,total_ap_e,total_ap_p,total_err_e,total_err_p,total_ndcg_e,total_ndcg_p,delta_list,_,_,_ = check_performance(combinations)\n",
    "print(\"\\nOnline evaluation with random click model and balanced interleaving: \")\n",
    "#Random click model has a lot of ties bacause often there are 0 clicks and we can also have both 1 or 2 clicks.\n",
    "wins_rcm = run_interleaving_rcm(N,1)\n",
    "print(\"\\nOnline evaluation with random click model and team based interleaving: \")\n",
    "#Random click model has a lot of ties bacause often there are 0 clicks and we can also have both 1 or 2 clicks.\n",
    "wins_rcm_team_based = run_interleaving_rcm(N,2)\n",
    "print(\"\\nOnline evaluation with SDBM and balanced interleaving: \")\n",
    "wins_sdbm = run_interleaving_SDBM(sigma_dict, alpha_dict,N,1)\n",
    "print(\"\\nOnline evaluation with SDBM and team based interleaving: \")\n",
    "wins_sdbm_team_based = run_interleaving_SDBM(sigma_dict, alpha_dict,N,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the offline evaluation methods, the outperformance is very close to 50% (only lower for average precision as ties are possible). This is expected as both E/P have the same relevancy combinations in the simulated combinations.\n",
    "\n",
    "When looking at the outcome of the offline and online evaluation methods, we notice a few interesting things. First of all, we see a clear difference between online and offline evaluation in the number of ties. In all three offline evaluation methods the number of ties is very small, while with online evaluation this is between 35 and 55%. This is caused by the fact that in offline evaluation the probability of a draw is very small as the values are real numbers. The probability of a draw with online evaluation is much larger since this happens whenever both E and P get the same amount of clicks which especially often happens when we have 0 clicks in total. In this simulation we count 0 clicks as a tie but it is mentioned seperately above to illustrate how often is occurs. \n",
    "\n",
    "That is also the reason why when using the random click model the number of draws is largest, as there the probability of a click is only around 13%, which causes a lot of cases where we have 0 clicks for both models and thus a tie. The probability of having 0 clicks, and thus a tie between E and P is approximatly $(1 - \\rho )^5 = (1-0.13)^5 \\approx 49$%\n",
    "\n",
    "This larger number of ties in online evaluation methods causes that we will have less cases where E outperforms P with only a very small (non significant) difference compared to offline evaluation. This can be seen as a benefit, as we will have fewer cases where we replace an existing system with a system that is not actually better. On the other hand, when looking for small improvements it can be hard to find these with online evaluation (even when we increase the number of experiments to a large number)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion part 2) Agreement of offline/online measures<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a large matrix which contains all the combinations and the results of the offline/online evaluation.\n",
    "\n",
    "The offline evaluation metrics are given as deltas between E and P (where a positive delta means score E was higher).\n",
    "\n",
    "The online evaluation metrics are given as $\\frac{wins_E}{N}$, which indicates how well an algorithm performs in online evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we put all results in one big matrix (so all deltas and online evalution metrics)\n",
    "matrix = np.zeros((len(combinations),7))\n",
    "for row in range(0,matrix.shape[0]):\n",
    "    for col in range(0,7):\n",
    "        if col == 0:\n",
    "            matrix[row][col] = delta_list[row][0]\n",
    "        if col == 1:\n",
    "            matrix[row][col] = delta_list[row][1]\n",
    "        if col == 2:\n",
    "            matrix[row][col] = delta_list[row][2]\n",
    "        if col == 3:\n",
    "            matrix[row][col] = wins_rcm[row][0]\n",
    "        if col == 4:\n",
    "            matrix[row][col] = wins_rcm_team_based[row][0]\n",
    "        if col == 5:\n",
    "            matrix[row][col] = wins_sdbm[row][0]\n",
    "        if col == 6:\n",
    "            matrix[row][col] = wins_sdbm_team_based[row][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Correlation matrix between offline/online evaluation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "absolute_matrix = np.copy(matrix)\n",
    "for row in range(0,matrix.shape[0]):\n",
    "    for col in range(0,3):\n",
    "        if matrix[row][col] > 0:\n",
    "            absolute_matrix[row][col] = 1\n",
    "        if matrix[row][col] < 0:\n",
    "            absolute_matrix[row][col] = 0\n",
    "        else: #It is a draw, so we win in 50% of the cases and therefore set the value to 0.5\n",
    "            absolute_matrix[row][col] = 0.5\n",
    "%matplotlib inline\n",
    "df = pd.DataFrame(absolute_matrix)\n",
    "df_corr = df.corr()\n",
    "ticklabels=[\"AP\",\"ERR\",\"NDCG\",\"RCM-B\",\"RCM-T\",\"SDBM-B\",\"SDBM-T\"]\n",
    "sns.heatmap(df_corr, annot=True,cmap=\"Blues\",yticklabels=ticklabels,xticklabels=ticklabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the heatmap above we see the correlation between all (both offline and online) evaluation methods based on all combinations. We did this by changing the offline evaluation methods based on the delta value for each metric. If the value was positive, so E outperformed P we made this value equal to 1. Whenever, the delta was negative, so P outperformed E, the value woul be set to zero. Finally, we then was a tie the value is set to 0.5. For the online evaluation metrics, we have the percentage of times E outperformed P from the N trials as value. \n",
    "\n",
    "There are several interesting things we notice. First of all, the random click model has a extremely low correlation. This is due to the fact that this model is totally random and therefore every iteration the outcome will have no correlation with the previous outcome. Secondly, we see that the highest correlation between an online and offline meausure is between SDBM-T and NDCG, where the correlation coefficient is equal to 0.53. The highest correlation between offline measures is between NDCG and the average precision and between online measures this is of course between the SDBM with the two different forms of interleaving. Finally, interestingly we see that the ERR has a very low correlation with all other metrics, which could indicate that in this setting this metric is less informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Random click model <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#AP versus percentage wins RCM team draft interleaving\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(matrix[:,0], matrix[:,3], s=70, alpha=0.003)\n",
    "plt.xlabel('Difference between AP of E and P.')\n",
    "plt.ylabel('Percentage of times E wins.')\n",
    "plt.title('Average precision versus wins of E')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(matrix[:,1], matrix[:,3], s=70, alpha=0.003)\n",
    "plt.xlabel('Difference between ERR of E and P.')\n",
    "plt.ylabel('Percentage of times E wins.')\n",
    "plt.title('ERR versus wins of E')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(matrix[:,2], matrix[:,3], s=70, alpha=0.003)\n",
    "plt.xlabel('Difference between nDCG@K of E and P.')\n",
    "plt.ylabel('Percentage of times E wins.')\n",
    "plt.title('nDCG@K versus wins of E')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the random click model we can see that there is no correlation between offline scoring and online evaluation. This is in line with out expectations as the relevance of the documents is not considered when clicking a document, therefore winning of E is random. \n",
    "\n",
    "The spread in the plots is merely a result of a random process (the process of clicking and therefore winning of E) and difference in relevance scoring based on the relevancy of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Simple Dynamic Bayesian Network<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AP versus percentage wins Simple Dynamic Bayesian Model team draft interleaving\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(matrix[:,0], matrix[:,6], s=70, alpha=0.003)\n",
    "plt.xlabel('Difference between AP of E and P.')\n",
    "plt.ylabel('Percentage of times E wins.')\n",
    "plt.title('Average precision versus wins of E')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(matrix[:,1], matrix[:,6], s=70, alpha=0.003)\n",
    "plt.xlabel('Difference between ERR of E and P.')\n",
    "plt.ylabel('Percentage of times E wins.')\n",
    "plt.title('ERR versus wins of E')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(matrix[:,2], matrix[:,6], s=70, alpha=0.003)\n",
    "plt.xlabel('Difference between nDCG@K of E and P.')\n",
    "plt.ylabel('Percentage of times E wins.')\n",
    "plt.title('nDCG@K versus wins of E')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation plots above, we could already tell that the ERR was uncorrelated with the online team draft interleaving. This means that this metric is a bad predictor for online performance.  \n",
    "\n",
    "nDCG@K is the best indicator of online performance as it is highly correlated with percentage of wins for E (correlation of 0.52)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion part 3) Statistical significance of results</b>\n",
    "\n",
    "For Statistical Significance testing we want to see whether offline/online evaluation metrics agree on a certain algorithm being better. We use the T-test to determine the statistical significance.\n",
    "\n",
    "All the offline evaluations have a different order of magnitude, we normalize them to compare them. A value of 1 is used if they predict E is the winner, 0.5 is used for a tie, 0 for a win for P. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from collections import Counter\n",
    "def t_test(online):\n",
    "    if online == True:\n",
    "        ap_winner = []\n",
    "        err_winner = []\n",
    "        ndcg_winner = []\n",
    "        #Here we compare the three offline evaluation metrics to check if they agree based on statistical significance.\n",
    "        _,_,_,_,_,_,_,_,_,_,ap_total,err_total,ndcg_total = check_performance(combinations,False)\n",
    "        assert (len(ap_total) == len(err_total) == len(ndcg_total))\n",
    "\n",
    "        for item in ap_total:\n",
    "            if item > 0:\n",
    "                ap_winner.append(1)\n",
    "            elif item < 0:\n",
    "                ap_winner.append(0)\n",
    "            else:\n",
    "                ap_winner.append(0.5)\n",
    "\n",
    "        for item in err_total:\n",
    "            if item > 0:\n",
    "                err_winner.append(1)\n",
    "            elif item < 0:\n",
    "                err_winner.append(0)\n",
    "            else:\n",
    "                err_winner.append(0.5)\n",
    "\n",
    "        for item in ndcg_total:\n",
    "\n",
    "            if item > 0:\n",
    "                ndcg_winner.append(1)\n",
    "            elif item < 0:\n",
    "                ndcg_winner.append(0)\n",
    "            else:\n",
    "                ndcg_winner.append(0.5)\n",
    "\n",
    "        #print(np.mean(ap_winner),np.mean(err_winner),np.mean(ndcg_winner))\n",
    "        #print(np.std(ap_winner),np.std(err_winner),np.std(ndcg_winner))\n",
    "        print(stats.ttest_ind(ap_winner,err_winner))\n",
    "        print(stats.ttest_ind(ap_winner,ndcg_winner))\n",
    "        print(stats.ttest_ind(err_winner,ndcg_winner))\n",
    "    else:\n",
    "        print(stats.ttest_ind(matrix[:,3],matrix[:,4])) #RCM for different interleaving methods\n",
    "        print(stats.ttest_ind(matrix[:,5],matrix[:,6])) #SDBM for different interleaving methods\n",
    "        print(stats.ttest_ind(matrix[:,3],matrix[:,5])) #Different click model same interleaving (balanced)\n",
    "        print(stats.ttest_ind(matrix[:,4],matrix[:,6])) #Different click model same interleaving (team-draf)\n",
    "t_test(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are in line with expectations, since all combinations are compared with each other and therefore we would expect the sum of all winner lists to be the same and therefore their means will be the same for all offline evaluation methods. This shows that we cannot reject the null hypothesis that they have the same underlying distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the online methods, the results are comparable as they all respresent the wins of E. Therefore we can use them in the T-test directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the online metrics we see that the difference between interleaving methods is not significant, so there the null hypothesis that these two samples are the same is not rejected. When comparing the different click models, we see that there the null hypothesis is rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrap test\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "\n",
    "def get_confidence_interval(data):#this function calculates a 95% confidence interval\n",
    "    a = 1.0*np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * sp.stats.t._ppf((1+0.95)/2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "def bootstrap(option): #this functions does the bootstrap test for 3 options. \n",
    "    # first is calculates the actual mean and confidence intervals \n",
    "    # Then it samples from the combinations and calculates what ratio of the cases it is ouside the confidence interval\n",
    "    diff_array = []\n",
    "    num_samples = 5000\n",
    "    out_of_bounds = 0\n",
    "    _,_,_,_,_,_,_,_,_,_,ap_total,err_total,ndcg_total = check_performance(combinations,False)\n",
    "    ap_total_flat = ap_total\n",
    "    err_total_flat = err_total\n",
    "    ndcg_total_flat = ndcg_total\n",
    "\n",
    "    assert (len(ap_total_flat) == len(err_total_flat) == len(ndcg_total_flat)) # check\n",
    "\n",
    "    ap_winner = []\n",
    "    err_winner = []\n",
    "    ndcg_winner = []\n",
    "\n",
    "    for item in ap_total_flat:\n",
    "        if item > 0:\n",
    "            ap_winner.append(1)\n",
    "        elif item < 0:\n",
    "            ap_winner.append(0)\n",
    "        else:\n",
    "            ap_winner.append(0.5)\n",
    "\n",
    "    for item in err_total_flat:\n",
    "        if item > 0:\n",
    "            err_winner.append(1)\n",
    "        elif item < 0:\n",
    "            err_winner.append(0)\n",
    "        else:\n",
    "            err_winner.append(0.5)\n",
    "\n",
    "    for item in ndcg_total_flat:\n",
    "\n",
    "        if item > 0:\n",
    "            ndcg_winner.append(1)\n",
    "        elif item < 0:\n",
    "            ndcg_winner.append(0)\n",
    "        else:\n",
    "            ndcg_winner.append(0.5)\n",
    "            \n",
    "    if option == 1:\n",
    "        actual_diff = [x1 - x2 for (x1, x2) in zip(ndcg_winner, err_winner)]\n",
    "        actual_mean,lower,higher = get_confidence_interval(actual_diff)\n",
    "    if option == 2:\n",
    "        actual_diff = [x1 - x2 for (x1, x2) in zip(ap_winner, err_winner)]\n",
    "        actual_mean,lower,higher = get_confidence_interval(actual_diff)\n",
    "    if option == 3:\n",
    "        actual_diff = [x1 - x2 for (x1, x2) in zip(ndcg_winner, err_winner)]\n",
    "        actual_mean,lower,higher = get_confidence_interval(actual_diff)\n",
    "    \n",
    "    # From here on it starts taking samples. It takes \"num_samples\" of size 10\n",
    "    for i in range(0,num_samples):\n",
    "        ap_total = []\n",
    "        err_total = []\n",
    "        ndcg_total = []\n",
    "        ap = []\n",
    "        err = []\n",
    "        ndcg = []\n",
    "        a = np.random.randint(0,len(combinations)-10)\n",
    "        combinations_check = combinations[a:a+10]\n",
    "        _,_,_,_,_,_,_,_,_,_,ap_total,err_total,ndcg_total = check_performance(combinations_check,False)\n",
    "        \n",
    "        assert (len(ap_total) == len(err_total) == len(ndcg_total))\n",
    "\n",
    "        ap_winner = []\n",
    "        err_winner = []\n",
    "        ndcg_winner = []\n",
    "\n",
    "        for item in ap_total:\n",
    "            if item > 0:\n",
    "                ap_winner.append(1)\n",
    "            elif item < 0:\n",
    "                ap_winner.append(0)\n",
    "            else:\n",
    "                ap_winner.append(0.5)\n",
    "\n",
    "        for item in err_total:\n",
    "            if item > 0:\n",
    "                err_winner.append(1)\n",
    "            elif item < 0:\n",
    "                err_winner.append(0)\n",
    "            else:\n",
    "                err_winner.append(0.5)\n",
    "\n",
    "        for item in ndcg_total:\n",
    "            if item > 0:\n",
    "                ndcg_winner.append(1)\n",
    "            elif item < 0:\n",
    "                ndcg_winner.append(0)\n",
    "            else:\n",
    "                ndcg_winner.append(0.5)\n",
    "        \n",
    "        if option == 1:\n",
    "            diff = np.mean(ndcg_winner) - np.mean(err_winner)\n",
    "            diff_array.append(diff)\n",
    "        if option == 2:\n",
    "            diff = np.mean(ap_winner) - np.mean(err_winner)\n",
    "            diff_array.append(diff)\n",
    "        if option == 3:\n",
    "            diff = np.mean(ap_winner) - np.mean(ndcg_winner)\n",
    "            diff_array.append(diff)\n",
    "\n",
    "    for element in diff_array: # check if it is inside our outside the confidence interval\n",
    "        if np.mean(element) < lower or np.mean(element) > higher:\n",
    "            out_of_bounds += 1\n",
    "    \n",
    "    return out_of_bounds/len(diff_array)\n",
    "            \n",
    "    \n",
    "print(bootstrap(1)) # option 1 compares the NDCG with ERR method\n",
    "print(bootstrap(2)) # option 2 compares the AP with ERR method\n",
    "print(bootstrap(3)) # option 3 compares the AP with NDCG method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Conclusion part 4) Differences in interleaving methods<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Although the assignment only asks for one interleaving method, we've implemented both team based and balanced interleaving. We see no clear difference in the number of times E outperforms P for both interleaving methods. This is the case since every element in the two lists that are to be interleaved is unique. Both methods produce an interleaved list that contains the same \"documents\", where the chance of the last document coming from E or P is 50% for both interleaving methods. Over a large number of experiments this effect of randomness does not produce significant results, in accordance to the law of large numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion part 5) Differences in click models<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We see a clear difference between the random click model and the simplified dynamic Bayesian model. When the random click model is used, we have a large number of ties caused by the large number of queries without clicks (which is explained before); therefore only in around 22% of the cases does either E outperform P or vice versa. With the SDBM the probability of a tie is less than 1%, this is because 65% of the queries has one click, and 34% of the queries does not have a click, leaving little room for a query with 2 clicks, resulting in a tie. This means that a (simulated) user is very easily satisfied after opening a document. \n",
    "\n",
    "\n",
    "The drawback of random click model seems to be that only 49% of the queries a document is clicked as every document has the same click probability. This rho parameter is an average over all positions, however in reality the first documents should have a much higher click probability. The SDBM is more complicated and is able to simulate clicks more realisticly: as a user examines the list from top to bottom, the first documents have a higher probability of being clicked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
