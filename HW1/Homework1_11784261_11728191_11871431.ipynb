{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretical part\n",
    "\n",
    "1a) P($m^{th}$ experiment gives significant result | m experiments lacking power to reject $H_0$)\n",
    "\n",
    "This probability of an experiment lacking the power to reject $H_0$ is ($1 -\\alpha  $) so the probability that it gets rejected is $\\alpha$. Assuming that consecutive experiments are  independent, the probability that the $m^{th}$ experiment gives significant result is: \n",
    "\n",
    "$(1-\\alpha)^{m-1}\\alpha$\n",
    "\n",
    "1b) P(at least one significant result | m experiments lacking power to reject $H_0$)\n",
    "\n",
    "This probability is given by: 1$-$ P(no significant results | m experiments lacking power to reject $H_0$).\n",
    "This then equals:\n",
    "\n",
    "$\n",
    "1 - (1 - \\alpha  )^{m}  \n",
    "$\n",
    "\n",
    "2) \n",
    "\n",
    "Suppose we have the following two lists that we want to interleave.\n",
    "\n",
    "RankA\n",
    "1 2 4\n",
    "\n",
    "RankB\n",
    "2 3 4\n",
    "\n",
    "Suppose 1 and 2 are clicked on 48% of the times, and 3 is clicked on 4% of the times. This means RankA gives us 96% satisfaction and RankB gives us 52% satisfaction, so RankA is the best algorithm.\n",
    "\n",
    "The coin tosses and interleaved lists are given below\n",
    "\n",
    "$\\textbf{AA}$\n",
    "\n",
    "1A 2B 4A (A wins 50% of the time)\n",
    "\n",
    "\n",
    "$\\textbf{AB}$\n",
    "\n",
    "1A 2B 3B (A wins 48% of the time)\n",
    "\n",
    "\n",
    "$\\textbf{BA}$\n",
    "\n",
    "2B 1A 4A (A wins 50% of the time)\n",
    "\n",
    "\n",
    "$\\textbf{BB}$\n",
    "\n",
    "2B 1A 3B (A wins 48% of the time)\n",
    "\n",
    "This means that the team draft interleaving is unfair to the better algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental part\n",
    "\n",
    "1) Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the next section, we first create a list of all combinations of relevances. We use itertools.product which gives all possible combinations of a list in any order. Then we use permutation which gives all the combinations of experiment and production relevances. We use itertools.permutations to do this. This means there are no duplicate results in combinations (same E-relevances as P-relevances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'R'], ['N', 'N', 'N', 'N', 'HR'], ['N', 'N', 'N', 'R', 'N'], ['N', 'N', 'N', 'R', 'R'], ['N', 'N', 'N', 'R', 'HR'], ['N', 'N', 'N', 'HR', 'N'], ['N', 'N', 'N', 'HR', 'R'], ['N', 'N', 'N', 'HR', 'HR'], ['N', 'N', 'R', 'N', 'N']]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "values = ['N','R','HR'] #possible values of a prediction\n",
    "\n",
    "relevances = [] #relevances contains all combinations of N/R/HR with length 5\n",
    "for r in itertools.product(values, repeat=5):\n",
    "    relevances.append(list(r))\n",
    "print(relevances[:10]) #print first 10 relevance combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinations = [] #combinations contains all pairs of relevances\n",
    "for p in itertools.permutations(relevances, 2):\n",
    "    combinations.append(list(p)) #we use this to get rid of the permutations object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'R']]]\n"
     ]
    }
   ],
   "source": [
    "print(combinations[:10]) #show the first 10 permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Step 2: Implement Evaluation Measures (10 points)\n",
    "\n",
    "In the next section we take two assumptions:\n",
    "\n",
    "1) Value mapping for the prediction relevances are N=0, R=1, HR=2.\n",
    "\n",
    "2) The amount of relevant predictions (overall) is assumed to be the total amount of relevant (R or HR) docs in the prediction set. So we assume there is no overlap between articles in the predictions of E/P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "#the first binary evaluation methods: average precision\n",
    "numeric_map = {'N':0, 'R':1, 'HR':2} #we use this numeric map to map N/R/HR to a numeric value.\n",
    "prediction = ['R','HR','N','R','N'] #this is a sample prediction to test functions\n",
    "\n",
    "def count_rel(prediction1,prediction2): #this function counts the total amount of relevant docs in both results.\n",
    "    return sum(1 for i in prediction1 if i != 'N') + sum(1 for i in prediction2 if i != 'N')\n",
    "\n",
    "def average_precision(prediction, r):\n",
    "    ap = 0\n",
    "    relevant_preds = 0\n",
    "    for i in range(0,len(prediction)): #for every element of the prediction\n",
    "        if prediction[i] != 'N': #if it is a relevant one\n",
    "            relevant_preds += 1 #add one to the cumulative relevant documents\n",
    "            ap += relevant_preds/(i+1) #calculate precision cumulative\n",
    "    return ap/r #take the average.\n",
    "\n",
    "ap = average_precision(prediction, count_rel(prediction, prediction))\n",
    "print(ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement both multi-graded evaluation methods. \n",
    "\n",
    "The first is nDCG@k which requires a optimal prediction to normalize predictions. Here we will use the total amount of HR/R files to create an optimum prediction. Again we assume there is no overlap in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53641800576\n"
     ]
    }
   ],
   "source": [
    "#nDCG@K\n",
    "import numpy as np #numpy is amazing right?\n",
    "\n",
    "def generate_opt(prediction1, prediction2): #generate optimal sequences from two predictions\n",
    "    opt_pred = []\n",
    "    num_hr = sum(1 for i in prediction1 if i == 'HR') + sum(1 for i in prediction2 if i == 'HR')\n",
    "    num_r  = sum(1 for i in prediction1 if i == 'R') + sum(1 for i in prediction2 if i == 'R')\n",
    "    for i in range(min(num_hr,5)): #check if num_hr exceeds 5, fill with HR's\n",
    "        opt_pred.append('HR')\n",
    "    for i in range(min(5-num_hr,num_r)): #check if num_r exceeds the space left, will with R's\n",
    "        opt_pred.append('R')\n",
    "    for i in range(5-len(opt_pred)): #fill the rest with N\n",
    "        opt_pred.append('N')\n",
    "    return opt_pred\n",
    "\n",
    "def dcg_k(numeric_map, prediction, opt_pred, k):\n",
    "    dcg_opt = 0\n",
    "    dcg = 0\n",
    "    for i in range(0,k): #for the range until K, we sum both the optimum and prediction dcg\n",
    "        dcg_opt += (2**numeric_map[opt_pred[i]]-1)/np.log2(1+i+1)\n",
    "        dcg +=(2**numeric_map[prediction[i]]-1)/np.log2(1+i+1)\n",
    "    return dcg/dcg_opt #dcg is normalized compared to the optimum\n",
    "ndcg = dcg_k(numeric_map, prediction, generate_opt(prediction,prediction), 3) #K=3 is used in this example\n",
    "print(ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second one is ERR, this model does not need any assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.281982421875\n"
     ]
    }
   ],
   "source": [
    "#ERR\n",
    "def ERR(numeric_map, prediction):\n",
    "    err = 0\n",
    "    max_val = 2**max(list(numeric_map.values())) #we calculate the maximum possible value from numeric mapping.\n",
    "    thetas = [(2**numeric_map[p]-1)/max_val for p in prediction] #for every prediction, we estimate theta value\n",
    "    for i in range(0,len(prediction)):\n",
    "        pred_val = 1 #we start with 1 as we want to do cumulative multiplication\n",
    "        for j in range(0,i): #loop back over earlier predictions\n",
    "            pred_val *= (1-thetas[j])*thetas[i] \n",
    "        pred_val *= 1/(i+1)\n",
    "        err += pred_val #add all values of individual predictions\n",
    "    return err\n",
    "err = ERR(numeric_map, prediction)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Step 3: Calculate the ùõ•measure (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we calculate the delta measure for the three offline performance measures.\n",
    "import math\n",
    "def check_performance(combinations):\n",
    "    #Here we initialize the counters for statistics\n",
    "    ap_counter = 0\n",
    "    ap_tie_counter = 0\n",
    "    ap_delta = []\n",
    "    total_ap_e = []\n",
    "    total_ap_p = []\n",
    "    err_counter = 0\n",
    "    err_tie_counter = 0\n",
    "    err_delta = []\n",
    "    total_err_e = []\n",
    "    total_err_p = []\n",
    "    ndcg_counter = 0\n",
    "    ndcg_tie_counter = 0\n",
    "    ndcg_delta = []\n",
    "    total_ndcg_e = []\n",
    "    total_ndcg_p = []\n",
    "    \n",
    "    total_count = len(combinations) #total amount of combinations we want to loop over\n",
    "    k = 5 #K is used for nDCG@K\n",
    "    for s in combinations:\n",
    "        prediction_e = s[0] #split the predictions\n",
    "        prediction_p = s[1]\n",
    "\n",
    "        r = count_rel(prediction_e, prediction_p) #count relevant docs for both predictions\n",
    "        ap_e, ap_p = average_precision(prediction_e, r), average_precision(prediction_p, r) #calculate ap\n",
    "        if ap_e > ap_p: #we only want to look at cases where e>p\n",
    "            ap_counter += 1\n",
    "            ap_delta.append(ap_e-ap_p) #we append the difference between e and p.\n",
    "        if ap_e == ap_p: #we also want to track ties\n",
    "            ap_tie_counter += 1\n",
    "        total_ap_e.append(ap_e)\n",
    "        total_ap_p.append(ap_p)\n",
    "        s.append(ap_e-ap_p)\n",
    "        \n",
    "        ERR_e, ERR_p = ERR(numeric_map, prediction_e), ERR(numeric_map, prediction_p) #calculate ERR scores\n",
    "        if ERR_e > ERR_p: #same as before\n",
    "            err_counter += 1\n",
    "            err_delta.append(ERR_e-ERR_p)\n",
    "        if ERR_e == ERR_p:\n",
    "            err_tie_counter += 1\n",
    "        total_err_e.append(ERR_e)\n",
    "        total_err_p.append(ERR_p)\n",
    "        s.append(ERR_e-ERR_p)\n",
    "        \n",
    "        opt_prediction = generate_opt(prediction_e,prediction_p) #first we need the optimal prediction for normalization\n",
    "        ndcg_e = dcg_k(numeric_map, prediction_e, opt_prediction, k) #calculate scoring for e\n",
    "        ndcg_p = dcg_k(numeric_map, prediction_p, opt_prediction, k) #calculate scoring for p\n",
    "        if ndcg_e > ndcg_p: #same as before\n",
    "            ndcg_counter += 1\n",
    "            ndcg_delta.append(ndcg_e-ndcg_p)\n",
    "        if ndcg_e == ndcg_p:\n",
    "            ndcg_tie_counter += 1\n",
    "        total_ndcg_e.append(ndcg_e)\n",
    "        total_ndcg_p.append(ndcg_p)\n",
    "        s.append(ndcg_e-ndcg_p)\n",
    "        \n",
    "    print(\"Percentage of cases that E outperforms P with average precision measure:\",round((ap_counter/total_count),2), \" and percentage of ties is: \",round((ap_tie_counter/total_count),2))\n",
    "    print(\"Percentage of cases that E outperforms P with ERR measure:\",round((err_counter/total_count),2),\"and percentage of ties is: \",round((err_tie_counter/total_count),2))\n",
    "    print(\"Percentage of cases that E outperforms P with NDCG measure:\",round((ndcg_counter/total_count),2),\"and percentage of ties is: \",round((ndcg_tie_counter/total_count),2))\n",
    "  \n",
    "    return ap_delta,err_delta,ndcg_delta,total_ap_e,total_ap_p,total_err_e,total_err_p,total_ndcg_e,total_ndcg_p,s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Implement Interleaving (15 points)\n",
    "We implemented both team based and balanced interleaving as this allows us to get a better insight in bias of both techniques. This will be discussed in the conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I is ['N', 'N', 'N', 'R', 'HR']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tie'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def get_A_first(): #This is a function that determines is ranking A goes first or not\n",
    "    A = np.random.uniform() # Take a random uniform number between 0 and 1    \n",
    "    if A > 0.5: \n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def balanced_interleaving(s, cutoff = False):\n",
    "    \n",
    "    ranking_A = s[0]\n",
    "    ranking_B = s[1]\n",
    "    \n",
    "    # Initialize\n",
    "    I = []\n",
    "    k_a, k_b = 0,0\n",
    "        \n",
    "    A_first = get_A_first() #Find out if A or B goes first\n",
    "    \n",
    "    # We assume that rankA and rankB contain 10 unique documents\n",
    "    # That is why we can cast rankA and rankB to a dict\n",
    "    # This makes it easier to return a list of length 9, while adhering to the pseudo code from the slides\n",
    "    \n",
    "    rankA = {}\n",
    "    rankB = {}\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]\n",
    "    \n",
    "    for i in A:\n",
    "        rankA[i] = ranking_A[i]\n",
    "        \n",
    "    for j in B:\n",
    "        rankB[j] = ranking_B[j-5] \n",
    "        \n",
    "    # This code just follows the pseudo code from the slides\n",
    "    while k_a+1 <= len(ranking_A) and k_b+1 <= len(ranking_B):\n",
    "        if (k_a < k_b) or ((k_a == k_b) and A_first):\n",
    "            if A[k_a] not in I:\n",
    "                I.append(A[k_a])\n",
    "            k_a += 1\n",
    "            \n",
    "        else:\n",
    "            if B[k_b] not in I:\n",
    "                I.append(B[k_b])\n",
    "            k_b += 1\n",
    "             \n",
    "    # I is now filled with unique indices, we now have to convert these back to labels\n",
    "    I_ids = copy.copy(I)\n",
    "    for i in range(0,len(I)):\n",
    "        try:\n",
    "            I[i] = rankA[I[i]]\n",
    "        except:\n",
    "            I[i] = rankB[I[i]]\n",
    "    \n",
    "    if cutoff: #Cut-off to make the lenght of the list the same length as A and B\n",
    "        return I[:len(ranking_A)], I_ids[:len(ranking_A)], ranking_A, ranking_B\n",
    "    else:\n",
    "        return I, I_ids, ranking_A, ranking_B\n",
    "    \n",
    "def team_based_interleaving(s, cutoff = False):\n",
    "    \n",
    "    ranking_A = s[0]\n",
    "    ranking_B = s[1]\n",
    "    \n",
    "    # Initialize\n",
    "    I = []\n",
    "    k_a, k_b = 0,0\n",
    "    \n",
    "    # We assume that rankA and rankB contain 10 unique documents\n",
    "    # That is why we can cast rankA and rankB to a dict\n",
    "    # This makes it easier to return a list of length 9, while adhering to the pseudo code from the slides\n",
    "    \n",
    "    rankA = {}\n",
    "    rankB = {}\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]\n",
    "    \n",
    "    for i in A:\n",
    "        rankA[i] = ranking_A[i]\n",
    "        \n",
    "    for j in B:\n",
    "        rankB[j] = ranking_B[j-5] \n",
    "        \n",
    "    # This code just follows the pseudo code from the slides\n",
    "    teamA = 0\n",
    "    teamB = 0\n",
    "    \n",
    "    for i in range(0,len(ranking_A)):\n",
    "        A_first = get_A_first() # Flip a coin each time we go again\n",
    "        if A_first:\n",
    "            for idx in A: #Loop over A\n",
    "                if idx not in I: #Top result in A not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "            for idx in B: #Loop over B\n",
    "                if idx not in I: #Top result in B not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "        else:\n",
    "            for idx in B: #Loop over B\n",
    "                if idx not in I: #Top result in B not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "            for idx in A: #Loop over A\n",
    "                if idx not in I: #Top result in A not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "                \n",
    "    # I is now filled with unique indices, we now have to convert these back to labels\n",
    "    I_ids = copy.copy(I)\n",
    "    for i in range(0,len(I)):\n",
    "        try:\n",
    "            I[i] = rankA[I[i]]\n",
    "        except:\n",
    "            I[i] = rankB[I[i]]\n",
    "    \n",
    "    if cutoff: #Cut-off to make the lenght of the list the same length as A and B\n",
    "        return I[:len(ranking_A)], I_ids[:len(ranking_A)], ranking_A, ranking_B\n",
    "    else:\n",
    "        return I, I_ids, ranking_A, ranking_B\n",
    "\n",
    "def define_winner(clicks,I_ids):\n",
    "            \n",
    "    clicks_A = 0 # Number of clicks from result A\n",
    "    clicks_B = 0 # Number or clicks from result B\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]        \n",
    "            \n",
    "    for click in range(0,len(clicks)): # Loop over the clicks\n",
    "        if clicks[click]:\n",
    "            if I_ids[click] in A:\n",
    "                clicks_A += 1\n",
    "            elif I_ids[click] in B:\n",
    "                clicks_B += 1\n",
    "            \n",
    "    if clicks_A > clicks_B:\n",
    "        return \"A\"\n",
    "    elif clicks_B > clicks_A:\n",
    "        return \"B\"\n",
    "    else: \n",
    "        return \"Tie\"\n",
    "            \n",
    "test_set = combinations[12347]\n",
    "clicks = [True,True,False,False,False]\n",
    "\n",
    "# I, I_ids, rank_A, rank_B = balanced_interleaving(test_set, True)\n",
    "I, I_ids, rank_A, rank_B = team_based_interleaving(test_set, True)\n",
    "print ('I is',I)\n",
    "\n",
    "define_winner(clicks,I_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Step 5: Implement User Clicks Simulation (15 points)\n",
    " First we will need to import the YandexRel dataset. Then we implement the random click model whereafter we implement a simplified Dynamic Baseysian Network Model.\n",
    " \n",
    "In the query data, we assume only one result page is observed and that each result page is a \"new query\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example query, results and clicks:\n",
      "Query ID: 9\n",
      "Query results: [13, 70, 66, 94, 50, 104, 29, 21, 89, 85]\n",
      "Query clicks: [104, 21]\n",
      "We have  42652  answers/click sequences in total!\n"
     ]
    }
   ],
   "source": [
    "#read search query data\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def read_data():\n",
    "\n",
    "    answers = []\n",
    "    query_ids = []\n",
    "    clicks = []\n",
    "    click = []\n",
    "    last_type = 'C'\n",
    "\n",
    "    with open('YandexRelPredChallenge.txt') as f:\n",
    "        for line in f:\n",
    "            vals = re.split(r'\\t+', line.rstrip())\n",
    "            line_type = vals[2] #we look at the type of data line\n",
    "            if line_type == 'Q': #if type is query, we append the query.\n",
    "                if len(click) > 0: #we append clicks of last query before we go further\n",
    "                    clicks.append(click)\n",
    "                    click = []\n",
    "                answers.append(list(map(int, vals[5:])))\n",
    "                query_ids.append(int(vals[3]))\n",
    "            if last_type == 'Q' and line_type == 'Q': #If last type also was query there are no clicks  \n",
    "                clicks.append([])\n",
    "            elif line_type == 'C':\n",
    "                click.append(int(vals[3]))\n",
    "            last_type = vals[2]\n",
    "        clicks.append(click) #we shall not forget the last click sequence...\n",
    "    \n",
    "    return answers,query_ids,clicks\n",
    "\n",
    "answers,query_ids,clicks = read_data()\n",
    "\n",
    "print('Example query, results and clicks:')\n",
    "print(\"Query ID:\", query_ids[5])\n",
    "print(\"Query results:\", answers[5])\n",
    "print(\"Query clicks:\", clicks[5])\n",
    "print('We have ',len(clicks),' answers/click sequences in total!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the random click model, the number of clicks need to be divided by the number of docs shown to a user. The function returns the parameter rho which indicates the click probability per result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Yandex dataset the probability that a (random) document gets clicked is: 0.13445559411047547\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Function to determine rho parameter of random click model given set of documents and clicks\n",
    "def rcm(documents,clicks):\n",
    "    unique_docs = []\n",
    "    unique_clicks = []\n",
    "    \n",
    "    assert(len(documents) == len(clicks))\n",
    "    \n",
    "    #First we find all unique documents to get the count\n",
    "    for d in documents:\n",
    "        for e in d:\n",
    "            unique_docs.append(e)\n",
    "\n",
    "    #number_of_docs = len(Counter(unique_docs).keys())\n",
    "    number_of_docs = len(unique_docs)\n",
    "    #Now we determine for the total number of cliks\n",
    "    for c in clicks:\n",
    "        for e in c:\n",
    "            unique_clicks.append(e)\n",
    "        \n",
    "    #number_of_clicks = len(Counter(unique_clicks).keys())\n",
    "    number_of_clicks = len(unique_clicks)\n",
    "    rho = number_of_clicks/number_of_docs\n",
    "    #print(number_of_clicks,number_of_docs)\n",
    "    return rho\n",
    "\n",
    "print(\"In the Yandex dataset the probability that a (random) document gets clicked is:\", rcm(answers,clicks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement the Dynamic Bayesian Network Model which is actually the so-called simplified DBN model (SDBN), since we make the assumption that $\\gamma = 1$. We do this, because by doing so we can easily estimate the model parameters and it has been shown that the predictive power of this model is similar to the not simplified version.\n",
    "\n",
    "First we calculate sigma, which is determined for every query document pair. The sigma can be calculated by dividing the amount of times a document was the last document clicked (only way which a document was satisfying when $\\gamma=1$) divided by the amount of times the document was clicked for a specific query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dynamic Bayesian network model\n",
    "#first we will look at sigma, as we can derive this by MLE directly as the simplification of gamma=1 is made.\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "query_clicks = {} #here we save a dict of clicks per query.\n",
    "query_sigma = {} #here we save a dict of sigma's per document per query.\n",
    "for q in np.unique(query_ids)[:100]: #look at all distinct queries\n",
    "    indices = [i for i, j in enumerate(query_ids) if j == q]\n",
    "    query_clicks[q] = list(itemgetter(*indices)(clicks)) #look at clicks at those indices.\n",
    "    last_clicks = [] #track last clicks\n",
    "    for i in query_clicks[q]:\n",
    "        if isinstance(i, int): #these rules are because sometimes the query_clicks is not a list of lists.\n",
    "            last_clicks.append(query_clicks[q][-1])\n",
    "            break\n",
    "        elif len(i) > 0:\n",
    "            last_clicks.append(i[-1])\n",
    "        else:\n",
    "            last_clicks.append(0)\n",
    "    last_counter = Counter(last_clicks)\n",
    "    del last_counter[0] #get rid of the zeros as they were dummies\n",
    "    try:\n",
    "        click_counter = Counter(list(itertools.chain(*query_clicks[q]))) #in case of list of lists\n",
    "    except:\n",
    "        click_counter = Counter(query_clicks[q]) #in case of a single list.\n",
    "    sigma_dict = dict(Counter({k:last_counter[k]/click_counter[k] for k in click_counter})) #divide last_counter by click_counter to get sigma\n",
    "    query_sigma[q] = sigma_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the sigma's for the SDBN, however we cannot use these sigma's to evaluate e/p as we cannot link queries/documents. To be able to use the calculated sigma's, we will devide document query combinations in 3 parts: Not relevant, relevant and highly relevant (33% of query-document pairs each) and then we calculate the average sigma per group to use in the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HR': 0.98722901516920358, 'N': 0.046705599807594686, 'R': 0.54446941082870659}\n"
     ]
    }
   ],
   "source": [
    "sigmas = []\n",
    "for key in query_sigma:\n",
    "    [sigmas.append(i) for i in list(query_sigma[key].values())] #append all sigma values into a list\n",
    "sigmas = np.sort(sigmas) #sort the list\n",
    "pairs = len(sigmas) #look at length of the list\n",
    "\n",
    "sigma_dict = {'N': np.mean(sigmas[:int(pairs/3)]), 'R': np.mean(sigmas[int(pairs/3):-int(pairs/3)]), 'HR': np.mean(sigmas[-int(pairs/3):])}\n",
    "print(sigma_dict) #show the results of the sigma dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to calculate alpha. In the simplified model, we know which items are examined (either all items or all items until the last read item). We can devide the number of clicks by the number of examines to calculate alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HR': 0.55290135325137568, 'N': 0.0, 'R': 0.032043953614566648}\n"
     ]
    }
   ],
   "source": [
    "#We already have query-clicks so now we must develop query-examines. \n",
    "#If clicks is empty, we have examined every document. If we clicked something, we examined until the last doc.\n",
    "query_alphas = {}\n",
    "for q in np.unique(query_ids)[:100]: #we look at distinct queries again\n",
    "    examines = []\n",
    "    indices = [i for i, j in enumerate(query_ids) if j == q]\n",
    "    for i in indices:\n",
    "        answer = answers[i] #look at the answers\n",
    "        click = clicks[i] #we do not want to double track when someone clicks twice.\n",
    "        if len(click) == 0:\n",
    "            [examines.append(i) for i in answer] #user does not click, all results are appended to examines\n",
    "        else:\n",
    "            try:\n",
    "                last_click = max([answer.index(i) for i in click]) #we look at the click with the max index\n",
    "                [examines.append(i) for i in answer[:last_click+1]] #we append all documents which are before the last click\n",
    "            except:\n",
    "                error = True\n",
    "    examines_counter = Counter(examines) #we make a counter of the examines\n",
    "    try:\n",
    "        click_counter = Counter(list(itertools.chain(*query_clicks[q])))\n",
    "    except:\n",
    "        click_counter = Counter(query_clicks[q])\n",
    "    #now we want to divide the clicks by the examines to get the attractive parameter\n",
    "    query_alphas[q] = dict(Counter({k:click_counter[k]/examines_counter[k] for k in examines_counter}))\n",
    "\n",
    "#now we do the same trick as before as we cannot link documents/queries. So we pick three categories.\n",
    "alphas = []\n",
    "for key in query_alphas:\n",
    "    [alphas.append(i) for i in list(query_alphas[key].values())]\n",
    "alphas = np.sort(alphas)\n",
    "pairs = len(alphas)\n",
    "\n",
    "alpha_dict = {'N': np.mean(alphas[:int(pairs/3)]), 'R': np.mean(alphas[int(pairs/3):-int(pairs/3)]), 'HR': np.mean(alphas[-int(pairs/3):])}\n",
    "print(alpha_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Simulate Interleaving Experiment (10 points)\n",
    "Now we want to simulate based on both click models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run experiment n times with balanced interleaving and the random click model\n",
    "def run_interleaving_rcm(N,interleaving):\n",
    "    \n",
    "    # Initialize random click model first\n",
    "    documents,_,clicks = read_data()\n",
    "    rho = rcm(documents,clicks)\n",
    "    \n",
    "    A_winner = 0\n",
    "    B_winner = 0\n",
    "    Tie = 0\n",
    "    win_list = []\n",
    "    for combination in combinations:\n",
    "        combinationA_winner = 0\n",
    "        combinationB_winner = 0\n",
    "        if(interleaving == 1):\n",
    "            I,I_ids,rank_A,rank_B = balanced_interleaving(combination,True)\n",
    "        elif(interleaving == 2):\n",
    "            I,I_ids,rank_A,rank_B = team_based_interleaving(combination,True)\n",
    "        else:\n",
    "            print(\"Please use 1 for balanced interleaving and 2 for team based interleaving.\")\n",
    "        # The cutoff bool determines the lenth of the interleaved list\n",
    "        # If set to false, the length is rank_A + rank_B - 1\n",
    "        # Is set to true, the length is of rank_A\n",
    "        \n",
    "        # We ignore order since it is a stochastic process anyway  \n",
    "        for n in range(0,N): #We decided to keep the interleaved list equal across the N trials.\n",
    "            clicks = []\n",
    "            for i in range(0,len(I)):\n",
    "                random_variable = np.random.uniform()\n",
    "\n",
    "                if random_variable < rho: #we randomly determine a click with odds rho\n",
    "                    clicks.append(True)\n",
    "                else:\n",
    "                    clicks.append(False)\n",
    "\n",
    "            winner = define_winner(clicks,I_ids) #define winner tells who is the winner based on clicks.\n",
    "            \n",
    "            if winner == \"A\":\n",
    "                A_winner += 1\n",
    "                combinationA_winner += 1\n",
    "            elif winner == \"B\":\n",
    "                B_winner += 1\n",
    "                combinationB_winner += 1\n",
    "            else:\n",
    "                Tie += 1\n",
    "        win_list.append([combinationA_winner/N,combinationB_winner/N,(N-(combinationA_winner+combinationB_winner))/N]) \n",
    "    \n",
    "    total = A_winner + B_winner + Tie    \n",
    "    print (\"Percentage of cases that E outperforms P:\",round(100*A_winner/total,2),\"and percentage of ties is: \",round(100*Tie/total,2))\n",
    "    #print (\"P outperforms E\",100*B_winner/total, \"percent of the time\")\n",
    "    #print (\"It is a tie\",100*Tie/total, \"percent of the time\")\n",
    "    return win_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run experiment n times with balanced interleaving and the SDB click model\n",
    "def run_interleaving_SDBM(sigma_dict, alpha_dict,N,interleaving):\n",
    "    A_winner = 0\n",
    "    B_winner = 0\n",
    "    Tie = 0\n",
    "    win_list = []\n",
    "    for combination in combinations:\n",
    "        combinationA_winner = 0\n",
    "        combinationB_winner = 0\n",
    "        if(interleaving == 1):\n",
    "            I,I_ids,rank_A,rank_B = balanced_interleaving(combination,True)\n",
    "        elif(interleaving == 2):\n",
    "            I,I_ids,rank_A,rank_B = team_based_interleaving(combination,True)\n",
    "        else:\n",
    "            print(\"Please use 1 for balanced interleaving and 2 for team based interleaving.\")\n",
    "        # The cutoff bool determines the lenth of the interleaved list\n",
    "        # If set to false, the length is rank_A + rank_B - 1\n",
    "        # Is set to true, the length is of rank_A\n",
    "        \n",
    "        for n in range(0,N):\n",
    "            clicks = []\n",
    "            satisfied = False\n",
    "\n",
    "            for i in range(0,len(I)):\n",
    "                relevance = I[i]\n",
    "                random_variable = np.random.uniform()\n",
    "\n",
    "                if random_variable < alpha_dict[relevance]: #here we check if the user is attracted\n",
    "                    if len(clicks) < len(I): #we cannot click more documents than results in list.\n",
    "                        clicks.append(True)\n",
    "                    random_variable_2 = np.random.uniform()\n",
    "                    if random_variable_2 < sigma_dict[relevance]: #here we look if the user was satisfied\n",
    "                        while len(clicks) < len(I):\n",
    "                            clicks.append(False) #if the user was satisfied, he does not click the rest of the list.\n",
    "                else:\n",
    "                    if len(clicks) < len(I):\n",
    "                        clicks.append(False)\n",
    "\n",
    "            winner = define_winner(clicks,I_ids)\n",
    "\n",
    "            if winner == \"A\":\n",
    "                A_winner += 1\n",
    "                combinationA_winner += 1\n",
    "            elif winner == \"B\":\n",
    "                B_winner += 1\n",
    "                combinationB_winner += 1\n",
    "            else:\n",
    "                Tie += 1\n",
    "        win_list.append([combinationA_winner/N,combinationB_winner/N,(N-(combinationA_winner+combinationB_winner))/N]) \n",
    "\n",
    "        \n",
    "    total = A_winner + B_winner + Tie    \n",
    "    print (\"Percentage of cases that E outperforms P:\",round(100*A_winner/total,2),\"and percentage of ties is: \",round(100*Tie/total,2))\n",
    "    return win_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Results and Analysis (30 points)\n",
    "First we will develop offline results of the combinations. Then we will develop statistics based on online evaluation by the two models (random click, SDBN) and the two interleaving methods (balanced/team based)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline evaluation: \n",
      "Percentage of cases that E outperforms P with average precision measure: 0.48  and percentage of ties is:  0.05\n",
      "Percentage of cases that E outperforms P with ERR measure: 0.5 and percentage of ties is:  0.0\n",
      "Percentage of cases that E outperforms P with NDCG measure: 0.5 and percentage of ties is:  0.0\n",
      "\n",
      "Online evaluation with random click model and balanced interleaving: \n",
      "Percentage of cases that E outperforms P: 22.16 and percentage of ties is:  55.81\n",
      "\n",
      "Online evaluation with random click model and team based interleaving: \n",
      "Percentage of cases that E outperforms P: 22.16 and percentage of ties is:  55.75\n",
      "\n",
      "Online evaluation with SDBM and balanced interleaving: \n",
      "Percentage of cases that E outperforms P: 32.67 and percentage of ties is:  34.52\n",
      "\n",
      "Online evaluation with SDBM and team based interleaving: \n",
      "Percentage of cases that E outperforms P: 32.65 and percentage of ties is:  34.44\n"
     ]
    }
   ],
   "source": [
    "print(\"Offline evaluation: \")\n",
    "ap,err,ndcg,total_ap_e,total_ap_p,total_err_e,total_err_p,total_ndcg_e,total_ndcg_p,delta_dict = check_performance(combinations)\n",
    "print(\"\\nOnline evaluation with random click model and balanced interleaving: \")\n",
    "#Random click model has a lot of ties bacause often there are 0 clicks and we can also have both 1 or 2 clicks.\n",
    "run_interleaving_rcm(5,1)\n",
    "print(\"\\nOnline evaluation with random click model and team based interleaving: \")\n",
    "#Random click model has a lot of ties bacause often there are 0 clicks and we can also have both 1 or 2 clicks.\n",
    "wins_rcm = run_interleaving_rcm(5,2)\n",
    "\n",
    "print(\"\\nOnline evaluation with SDBM and balanced interleaving: \")\n",
    "wins_sdbm = run_interleaving_SDBM(sigma_dict, alpha_dict,5,1)\n",
    "print(\"\\nOnline evaluation with SDBM and team based interleaving: \")\n",
    "run_interleaving_SDBM(sigma_dict, alpha_dict,5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=0.0, pvalue=1.0)\n",
      "Ttest_indResult(statistic=-3.0655144714229421e-13, pvalue=0.99999999999975542)\n",
      "Ttest_indResult(statistic=9.4888429388964605e-14, pvalue=0.99999999999992428) \n",
      "\n",
      "The delta average precision when E outperformed P has a mean of 0.251985571156 a standard deviation of 0.177610579289 and the maximum and minimum are 1.0 and 0.0047619047619 .\n",
      "\n",
      "The delta ERR when E outperformed P has a mean of 0.138850591743 a standard deviation of 0.108216781117 and the maximum and minimum are 0.429455566406 and 3.05175781246e-06 .\n",
      "\n",
      "The delta NDCG when E outperformed P has a mean of 0.28219244365 a standard deviation of 0.202560648236 and the maximum and minimum are 1.0 and 6.70863437456e-05 .\n"
     ]
    }
   ],
   "source": [
    "#Visualization results\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "print(stats.ttest_ind(total_ap_e,total_ap_p))\n",
    "print(stats.ttest_ind(total_err_e,total_err_p))\n",
    "print(stats.ttest_ind(total_ndcg_e,total_ndcg_p),\"\\n\")\n",
    "\n",
    "print(\"The delta average precision when E outperformed P has a mean of\", np.mean(ap), \"a standard deviation of\", np.std(ap), \"and the maximum and minimum are\", np.max(ap), \"and\", np.min(ap),\".\\n\")\n",
    "\n",
    "print(\"The delta ERR when E outperformed P has a mean of\", np.mean(err), \"a standard deviation of\",np.std(err), \"and the maximum and minimum are\", np.max(err), \"and\", np.min(err),\".\\n\")\n",
    "\n",
    "print(\"The delta NDCG when E outperformed P has a mean of\", np.mean(ndcg), \"a standard deviation of\",np.std(ndcg), \"and the maximum and minimum are\", np.max(ndcg), \"and\", np.min(ndcg),\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Conclusion offline and online evaluation methods</b>\n",
    "For the offline evaluation methods, the outperformance is very close to 50% (only lower for average precision as ties are possible). This is expected as both E/P have the same relevancy combinations in the simulated combinations.\n",
    "\n",
    "When looking at the outcome of the offline and online evaluation methods, we notice a few interesting things. First of all, we see a clear difference between online and offline evaluation in the number of ties. In all three offline evaluation methods the number of ties is very small, while with online evaluation this is between 35 and 55%. This is caused by the fact that in offline evaluation the probability of a draw is very small as the values are real numbers. The probability of a draw with online evaluation is much larger since this happens whenever both E and P get the same amount of clicks which especially often happens when we have 0 clicks in total. \n",
    "\n",
    "That is also the reason why when using the random click model the number of draws is largest, as there the probability of a click is only around 13%, which causes a lot of cases where we have 0 clicks for both models and thus a tie. The probability of having 0 clicks, and thus a tie between E and P is approximatly $(1 - \\rho )^5 = (1-0.13)^5 \\approx 50$%\n",
    "\n",
    "This larger number of ties in online evaluation methods causes that we will have less cases where E outperforms P with only a very small (non significant) difference compared to online evaluation. This can be seen as a benefit, as we will have fewer cases where we replace an existing system with a system that is not actually better. On the other hand, when looking for small improvements it can be hard to find these with online evaluation (even when we increase the number of experiments to a large number)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Conclusion interleaving methods</b>\n",
    "Although the assignment only asks for one interleaving method, we've implemented both team based and balanced interleaving. We see no clear difference in the number of times E outperforms P for both interleaving methods. This is the case since every element in the two lists that are to be interleaved is unique. Both methods produce an interleaved list that contains the same \"documents\", where the chance of the last document coming from E or P is 50% for both interleaving methods. Over a large number of experiments this effect of randomness does not produce significant results, in accordance to the law of large numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Conclusion click models</b>\n",
    "We see a clear difference between the random click model and the simplified dynamic Bayesian model. When the random click model is used, we have a large number of ties (which is explained before) therefore only in around 22.5% of the cases does either E outperform P or vice versa. With the SDBM we a lower probability of a draw at 34% and probability of 33% for outperformance.\n",
    "\n",
    "The drawback of random click model seems to be that only 50% of the queries a document is clicked as every document has the same click probability. This rho parameter is an average over all positions, however in reality the first documents should have a much higher click probability. The SDBM is more complicated and is able to simulate clicks more realisticly: as a user examines the list from top to bottom, the first documents have a higher probability of being clicked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
