{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretical part\n",
    "\n",
    "1a) P($m^{th}$ experiment gives significant result | m experiments lacking power to reject $H_0$)\n",
    "\n",
    "This probability of an experiment lacking the power to reject $H_0$ is ($1 -\\alpha  $) so the probability that it gets rejected is $\\alpha$. Assuming that consecutive experiments are  independent, the probability that the $m^{th}$ experiment gives significant result is: \n",
    "\n",
    "$\\alpha$\n",
    "\n",
    "TODO: KIEZEN WELKE VERSIE VAN HET ANTWOORD HIERBOVEN WE INLEVEREN\n",
    "\n",
    "1b) P(at least one significant result | m experiments lacking power to reject $H_0$)\n",
    "\n",
    "This probability is given by: 1$-$ P(no significant results | m experiments lacking power to reject $H_0$) +\n",
    "This then equals:\n",
    "\n",
    "$\n",
    "1 - (1 - \\alpha  )^{m}  \n",
    "$\n",
    "\n",
    "2) \n",
    "\n",
    "Suppose we have the following two lists that we want to interleave.\n",
    "\n",
    "RankA\n",
    "1 2 4\n",
    "\n",
    "RankB\n",
    "2 3 4\n",
    "\n",
    "Suppose 1 and 2 are clicked on 48% of the times, and 3 is clicked on 4% of the times. This means RankA gives us 96% satisfaction and RankB gives us 52% satisfaction, so RankA is the best algorithm.\n",
    "\n",
    "The coin tosses and interleaved lists are given below\n",
    "\n",
    "$\\textbf{AA}$\n",
    "\n",
    "1A 2B 4A (A wins 50% of the time)\n",
    "\n",
    "\n",
    "$\\textbf{AB}$\n",
    "\n",
    "1A 2B 3B (A wins 48% of the time)\n",
    "\n",
    "\n",
    "$\\textbf{BA}$\n",
    "\n",
    "2B 1A 4A (A wins 50% of the time)\n",
    "\n",
    "\n",
    "$\\textbf{BB}$\n",
    "\n",
    "2B 1A 3B (A wins 48% of the time)\n",
    "\n",
    "This means that the team draft interleaving is unfair to the better algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental part\n",
    "\n",
    "1) Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the next section, we first create a list of all combinations of relevances. We use itertools.product which gives all possible combinations of a list in any order. Then we use permutation which gives all the combinations of experiment and production relevances. We use itertools.permutations to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "values = ['N','R','HR'] #possible values of a prediction\n",
    "\n",
    "relevances = [] #relevances contains all combinations of N/R/HR with length 5\n",
    "for r in itertools.product(values, repeat=5):\n",
    "    relevances.append(list(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinations = [] #combinations contains all pairs of relevances\n",
    "for p in itertools.permutations(relevances, 2):\n",
    "    combinations.append(list(p)) #we use this to get rid of the permutations object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'R']]]\n"
     ]
    }
   ],
   "source": [
    "print(combinations[:10]) #show the first 10 combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Step 2: Implement Evaluation Measures (10 points)\n",
    "\n",
    "In the next section we take two assumptions:\n",
    "\n",
    "1) Values for the prediction relevances are N=0, R=1, HR=2\n",
    "\n",
    "2) The amount of relevant predictions (overall) is assumed to be the total amount of relevant (R or HR) docs in the prediction set. So we assume there is no overlap between articles in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "#the first binary evaluation methods: average precision\n",
    "numeric_map = {'N':0, 'R':1, 'HR':2} #we use this numeric map to map N/R/HR to a numeric value.\n",
    "prediction = ['R','HR','N','R','N'] #this is a sample prediction to test functions\n",
    "\n",
    "def count_rel(prediction1,prediction2):\n",
    "    return sum(1 for i in prediction1 if i != 'N') + sum(1 for i in prediction2 if i != 'N')\n",
    "\n",
    "def average_precision(prediction, r):\n",
    "    ap = 0\n",
    "    relevant_preds = 0\n",
    "    for i in range(0,len(prediction)):\n",
    "        if prediction[i] != 'N':\n",
    "            relevant_preds += 1\n",
    "            ap += relevant_preds/(i+1)\n",
    "    return ap/r\n",
    "\n",
    "ap = average_precision(prediction, count_rel(prediction, prediction))\n",
    "print(ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement both multi-graded evaluation methods. \n",
    "\n",
    "The first is nDCG@k which requires a optimal prediction to normalize predictions. Here we will use the total amount of HR/R files to create an optimum prediction. Again we assume there is no overlap in predictions.\n",
    "\n",
    "The second one is ERR, this model does not need any assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53641800576\n"
     ]
    }
   ],
   "source": [
    "#nDCG@K\n",
    "import numpy as np #Numpy is amazing right?\n",
    "\n",
    "def generate_opt(prediction1, prediction2): #generate optimal sequences from two predictions\n",
    "    opt_pred = []\n",
    "    num_hr = sum(1 for i in prediction1 if i == 'HR') + sum(1 for i in prediction2 if i == 'HR')\n",
    "    num_r  = sum(1 for i in prediction1 if i == 'R') + sum(1 for i in prediction2 if i == 'R')\n",
    "    for i in range(min(num_hr,5)): #check if num_hr exceeds 5, fill with HR's\n",
    "        opt_pred.append('HR')\n",
    "    for i in range(min(5-num_hr,num_r)): #check if num_r exceeds the space left, will with R's\n",
    "        opt_pred.append('R')\n",
    "    for i in range(5-len(opt_pred)): #fill the rest with N\n",
    "        opt_pred.append('N')\n",
    "    return opt_pred\n",
    "\n",
    "def dcg_k(numeric_map, prediction, opt_pred, k):\n",
    "    dcg_opt = 0\n",
    "    dcg = 0\n",
    "    for i in range(0,k): #for the range until K, we sum both the optimum and prediction dcg\n",
    "        dcg_opt += (2**numeric_map[opt_pred[i]]-1)/np.log2(1+i+1)\n",
    "        dcg +=(2**numeric_map[prediction[i]]-1)/np.log2(1+i+1)\n",
    "    return dcg/dcg_opt #dcg is normalized compared to the optimum\n",
    "ndcg = dcg_k(numeric_map, prediction, generate_opt(prediction,prediction), 3)\n",
    "print(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.281982421875\n"
     ]
    }
   ],
   "source": [
    "#ERR\n",
    "def ERR(numeric_map, prediction):\n",
    "    err = 0\n",
    "    max_val = 2**max(list(numeric_map.values()))\n",
    "    thetas = [(2**numeric_map[p]-1)/max_val for p in prediction]\n",
    "    for i in range(0,len(prediction)):\n",
    "        prod_val = 1\n",
    "        for j in range(0,i):\n",
    "            prod_val *= (1-thetas[j])*thetas[i]\n",
    "        prod_val *= 1/(i+1)\n",
    "        err += prod_val\n",
    "    return err\n",
    "err = ERR(numeric_map, prediction)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Step 3: Calculate the 𝛥measure (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we calculate the delta measure for the three offline performance measures.\n",
    "import math\n",
    "def check_performance(combinations):\n",
    "    ap_counter = 0\n",
    "    ap_tie_counter = 0\n",
    "    ap_delta = []\n",
    "    total_ap_e = []\n",
    "    total_ap_p = []\n",
    "    err_counter = 0\n",
    "    err_tie_counter = 0\n",
    "    err_delta = []\n",
    "    total_err_e = []\n",
    "    total_err_p = []\n",
    "    ndcg_counter = 0\n",
    "    ndcg_tie_counter = 0\n",
    "    ndcg_delta = []\n",
    "    total_ndcg_e = []\n",
    "    total_ndcg_p = []\n",
    "    \n",
    "    total_count = len(combinations)\n",
    "    k = 5\n",
    "    for s in combinations:\n",
    "      \n",
    "        prediction_e = s[0]\n",
    "        prediction_p = s[1]\n",
    "\n",
    "        \n",
    "        r = count_rel(prediction_e, prediction_p)\n",
    "        ap_e, ap_p = average_precision(prediction_e, r), average_precision(prediction_p, r)\n",
    "        if ap_e > ap_p:\n",
    "            ap_counter += 1\n",
    "            ap_delta.append(ap_e-ap_p)\n",
    "        if ap_e == ap_p:\n",
    "            ap_tie_counter += 1\n",
    "        total_ap_e.append(ap_e)\n",
    "        total_ap_p.append(ap_p)\n",
    "            \n",
    "        ERR_e, ERR_p = ERR(numeric_map, prediction_e), ERR(numeric_map, prediction_p)\n",
    "        if ERR_e > ERR_p:\n",
    "            err_counter += 1\n",
    "            err_delta.append(ERR_e-ERR_p)\n",
    "        if ERR_e == ERR_p:\n",
    "            err_tie_counter += 1\n",
    "        total_err_e.append(ERR_e)\n",
    "        total_err_p.append(ERR_p)\n",
    "        \n",
    "        opt_prediction = generate_opt(prediction_e,prediction_p)\n",
    "        ndcg_e = dcg_k(numeric_map, prediction_e, opt_prediction, k)\n",
    "        ndcg_p = dcg_k(numeric_map, prediction_p, opt_prediction, k)\n",
    "        if ndcg_e > ndcg_p:\n",
    "            ndcg_counter += 1\n",
    "            ndcg_delta.append(ndcg_e-ndcg_p)\n",
    "        if ndcg_e == ndcg_p:\n",
    "            ndcg_tie_counter += 1\n",
    "        total_ndcg_e.append(ndcg_e)\n",
    "        total_ndcg_p.append(ndcg_p)\n",
    "        \n",
    "    print(\"Percentage of cases that E outperforms P with average precision measure:\",round((ap_counter/total_count),2), \" and percentage of ties is: \",round((ap_tie_counter/total_count),2))\n",
    "    print(\"Percentage of cases that E outperforms P with ERR measure:\",round((err_counter/total_count),2),\"and percentage of ties is: \",round((err_tie_counter/total_count),2))\n",
    "    print(\"Percentage of cases that E outperforms P with NDCG measure:\",round((ndcg_counter/total_count),2),\"and percentage of ties is: \",round((ndcg_tie_counter/total_count),2))\n",
    "  \n",
    "    return ap_delta,err_delta,ndcg_delta,total_ap_e,total_ap_p,total_err_e,total_err_p,total_ndcg_e,total_ndcg_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the delta measure show that for all three offline performance measures we see in 50% of the cases that the experiment ranking outperforms the production ranking. This is in line with expectations, since the combinations (rankings) are random and therefore we would expect that in a little less than 50% of the cases the experiment outperforms the production (since we also have ties its slightly less than 50%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Implement Interleaving (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I is ['N', 'N', 'R', 'N', 'HR']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tie'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def get_A_first(): #This is a function that determines is ranking A goes first or not\n",
    "    A = np.random.uniform() # Take a random uniform number between 0 and 1    \n",
    "    if A > 0.5: \n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def balanced_interleaving(s, cutoff = False):\n",
    "    \n",
    "    ranking_A = s[0]\n",
    "    ranking_B = s[1]\n",
    "    \n",
    "    # Initialize\n",
    "    I = []\n",
    "    k_a, k_b = 0,0\n",
    "        \n",
    "    A_first = get_A_first() #Find out if A or B goes first\n",
    "    \n",
    "    # We assume that rankA and rankB contain 10 unique documents\n",
    "    # That is why we can cast rankA and rankB to a dict\n",
    "    # This makes it easier to return a list of length 9, while adhering to the pseudo code from the slides\n",
    "    \n",
    "    rankA = {}\n",
    "    rankB = {}\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]\n",
    "    \n",
    "    for i in A:\n",
    "        rankA[i] = ranking_A[i]\n",
    "        \n",
    "    for j in B:\n",
    "        rankB[j] = ranking_B[j-5] \n",
    "        \n",
    "    # This code just follows the pseudo code from the slides\n",
    "    while k_a+1 <= len(ranking_A) and k_b+1 <= len(ranking_B):\n",
    "        if (k_a < k_b) or ((k_a == k_b) and A_first):\n",
    "            if A[k_a] not in I:\n",
    "                I.append(A[k_a])\n",
    "            k_a += 1\n",
    "            \n",
    "        else:\n",
    "            if B[k_b] not in I:\n",
    "                I.append(B[k_b])\n",
    "            k_b += 1\n",
    "             \n",
    "    # I is now filled with unique indices, we now have to convert these back to labels\n",
    "    I_ids = copy.copy(I)\n",
    "    for i in range(0,len(I)):\n",
    "        try:\n",
    "            I[i] = rankA[I[i]]\n",
    "        except:\n",
    "            I[i] = rankB[I[i]]\n",
    "    \n",
    "    if cutoff: #Cut-off to make the lenght of the list the same length as A and B\n",
    "        return I[:len(ranking_A)], I_ids[:len(ranking_A)], ranking_A, ranking_B\n",
    "    else:\n",
    "        return I, I_ids, ranking_A, ranking_B\n",
    "    \n",
    "def team_based_interleaving(s, cutoff = False):\n",
    "    \n",
    "    ranking_A = s[0]\n",
    "    ranking_B = s[1]\n",
    "    \n",
    "    # Initialize\n",
    "    I = []\n",
    "    k_a, k_b = 0,0\n",
    "    \n",
    "    # We assume that rankA and rankB contain 10 unique documents\n",
    "    # That is why we can cast rankA and rankB to a dict\n",
    "    # This makes it easier to return a list of length 9, while adhering to the pseudo code from the slides\n",
    "    \n",
    "    rankA = {}\n",
    "    rankB = {}\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]\n",
    "    \n",
    "    for i in A:\n",
    "        rankA[i] = ranking_A[i]\n",
    "        \n",
    "    for j in B:\n",
    "        rankB[j] = ranking_B[j-5] \n",
    "        \n",
    "    # This code just follows the pseudo code from the slides\n",
    "    teamA = 0\n",
    "    teamB = 0\n",
    "    \n",
    "    for i in range(0,len(ranking_A)):\n",
    "        A_first = get_A_first() # Flip a coin each time we go again\n",
    "        if A_first:\n",
    "            for idx in A: #Loop over A\n",
    "                if idx not in I: #Top result in A not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "            for idx in B: #Loop over B\n",
    "                if idx not in I: #Top result in B not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "        else:\n",
    "            for idx in B: #Loop over B\n",
    "                if idx not in I: #Top result in B not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "            for idx in A: #Loop over A\n",
    "                if idx not in I: #Top result in A not yet in I\n",
    "                    I.append(idx)\n",
    "                    break\n",
    "                \n",
    "    # I is now filled with unique indices, we now have to convert these back to labels\n",
    "    I_ids = copy.copy(I)\n",
    "    for i in range(0,len(I)):\n",
    "        try:\n",
    "            I[i] = rankA[I[i]]\n",
    "        except:\n",
    "            I[i] = rankB[I[i]]\n",
    "    \n",
    "    if cutoff: #Cut-off to make the lenght of the list the same length as A and B\n",
    "        return I[:len(ranking_A)], I_ids[:len(ranking_A)], ranking_A, ranking_B\n",
    "    else:\n",
    "        return I, I_ids, ranking_A, ranking_B\n",
    "\n",
    "def define_winner(clicks,I_ids):\n",
    "            \n",
    "    clicks_A = 0 # Number of clicks from result A\n",
    "    clicks_B = 0 # Number or clicks from result B\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]        \n",
    "            \n",
    "    for click in range(0,len(clicks)): # Loop over the clicks\n",
    "        if clicks[click]:\n",
    "            if I_ids[click] in A:\n",
    "                clicks_A += 1\n",
    "            elif I_ids[click] in B:\n",
    "                clicks_B += 1\n",
    "            \n",
    "    if clicks_A > clicks_B:\n",
    "        return \"A\"\n",
    "    elif clicks_B > clicks_A:\n",
    "        return \"B\"\n",
    "    else: \n",
    "        return \"Tie\"\n",
    "            \n",
    "test_set = combinations[12347]\n",
    "clicks = [True,True,False,False,False]\n",
    "\n",
    "# I, I_ids, rank_A, rank_B = balanced_interleaving(test_set, True)\n",
    "I, I_ids, rank_A, rank_B = team_based_interleaving(test_set, True)\n",
    "print ('I is',I)\n",
    "\n",
    "define_winner(clicks,I_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Step 5: Implement User Clicks Simulation (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example query, results and clicks:\n",
      "Query ID: 9\n",
      "Query results: [13, 70, 66, 94, 50, 104, 29, 21, 89, 85]\n",
      "Query clicks: [104, 21]\n",
      "We have  42652  answers/click sequences in total!\n"
     ]
    }
   ],
   "source": [
    "#read search query data\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def read_data():\n",
    "\n",
    "    answers = []\n",
    "    query_ids = []\n",
    "    clicks = []\n",
    "    click = []\n",
    "    last_type = 'C'\n",
    "\n",
    "    with open('YandexRelPredChallenge.txt') as f:\n",
    "        for line in f:\n",
    "            vals = re.split(r'\\t+', line.rstrip())\n",
    "            line_type = vals[2] #we look at the type of data line\n",
    "            if line_type == 'Q': #if type is query, we append the query.\n",
    "                if len(click) > 0: #we append clicks of last query before we go further\n",
    "                    clicks.append(click)\n",
    "                    click = []\n",
    "                answers.append(list(map(int, vals[5:])))\n",
    "                query_ids.append(int(vals[3]))\n",
    "            if last_type == 'Q' and line_type == 'Q': #If last type also was query there are no clicks  \n",
    "                clicks.append([])\n",
    "            elif line_type == 'C':\n",
    "                click.append(int(vals[3]))\n",
    "            last_type = vals[2]\n",
    "        clicks.append(click) #we shall not forget the last click sequence...\n",
    "    \n",
    "    return answers,query_ids,clicks\n",
    "\n",
    "answers,query_ids,clicks = read_data()\n",
    "\n",
    "print('Example query, results and clicks:')\n",
    "print(\"Query ID:\", query_ids[5])\n",
    "print(\"Query results:\", answers[5])\n",
    "print(\"Query clicks:\", clicks[5])\n",
    "print('We have ',len(clicks),' answers/click sequences in total!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Yandex dataset the probability that a (random) document gets clicked is: 0.13445559411047547\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Function to determine rho parameter of random click model given set of documents and clicks\n",
    "def rcm(documents,clicks):\n",
    "    unique_docs = []\n",
    "    unique_clicks = []\n",
    "    \n",
    "    assert(len(documents) == len(clicks))\n",
    "    \n",
    "    #First we find all unique documents to get the count\n",
    "    for d in documents:\n",
    "        for e in d:\n",
    "            unique_docs.append(e)\n",
    "\n",
    "    #number_of_docs = len(Counter(unique_docs).keys())\n",
    "    number_of_docs = len(unique_docs)\n",
    "    #Now we determine for the total number of cliks\n",
    "    for c in clicks:\n",
    "        for e in c:\n",
    "            unique_clicks.append(e)\n",
    "        \n",
    "    #number_of_clicks = len(Counter(unique_clicks).keys())\n",
    "    number_of_clicks = len(unique_clicks)\n",
    "    rho = number_of_clicks/number_of_docs\n",
    "    #print(number_of_clicks,number_of_docs)\n",
    "    return rho\n",
    "\n",
    "print(\"In the Yandex dataset the probability that a (random) document gets clicked is:\", rcm(answers,clicks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dynamic Bayesian network model\n",
    "#first we will look at sigma, as we can derive this by MLE directly. We \n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "query_clicks = {}\n",
    "query_sigma = {}\n",
    "for q in np.unique(query_ids)[:100]:\n",
    "    indices = [i for i, j in enumerate(query_ids) if j == q]\n",
    "    query_clicks[q] = list(itemgetter(*indices)(clicks))\n",
    "    last_clicks = []\n",
    "    for i in query_clicks[q]:\n",
    "        if isinstance(i, int):\n",
    "            last_clicks.append(query_clicks[q][-1])\n",
    "            break\n",
    "        elif len(i) > 0:\n",
    "            last_clicks.append(i[-1])\n",
    "        else:\n",
    "            last_clicks.append(0)\n",
    "    last_counter = Counter(last_clicks)\n",
    "    del last_counter[0]\n",
    "    try:\n",
    "        click_counter = Counter(list(itertools.chain(*query_clicks[q])))\n",
    "    except:\n",
    "        click_counter = Counter(query_clicks[q])\n",
    "    sigma_dict = dict(Counter({k:last_counter[k]/click_counter[k] for k in click_counter}))\n",
    "    query_sigma[q] = sigma_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the sigma's for the DBNM, however we cannot use these sigma's to evaluate e/p as we cannot link queries/documents. To be able to use the calculated sigma's, we will devide document query combinations in 3 parts: Not relevant, relevant and highly relevant (33% of query-document pairs each) and then we calculate the average sigma per group to use in the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 0.046705599807594686, 'HR': 0.98722901516920358, 'R': 0.54446941082870659}\n"
     ]
    }
   ],
   "source": [
    "sigmas = []\n",
    "for key in query_sigma:\n",
    "    [sigmas.append(i) for i in list(query_sigma[key].values())]\n",
    "sigmas = np.sort(sigmas)\n",
    "pairs = len(sigmas)\n",
    "\n",
    "sigma_dict = {'N': np.mean(sigmas[:int(pairs/3)]), 'R': np.mean(sigmas[int(pairs/3):-int(pairs/3)]), 'HR': np.mean(sigmas[-int(pairs/3):])}\n",
    "print(sigma_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplified model, we know which items are examined (either all items or all items until the last read item). We can devide the number of clicks by the number of examines to calculate alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 0.0, 'HR': 0.55290135325137568, 'R': 0.032043953614566648}\n"
     ]
    }
   ],
   "source": [
    "#We already have query-clicks so now we must develop query-examines. \n",
    "#If clicks is empty, we have examined every document. If we clicked something, we examined until the last doc.\n",
    "query_alphas = {}\n",
    "for q in np.unique(query_ids)[:100]:\n",
    "    examines = []\n",
    "    indices = [i for i, j in enumerate(query_ids) if j == q]\n",
    "    for i in indices:\n",
    "        answer = answers[i]\n",
    "        click = clicks[i] #we do not want to double track when someone clicks twice.\n",
    "        if len(click) == 0:\n",
    "            [examines.append(i) for i in answer]\n",
    "        else:\n",
    "            try:\n",
    "                last_click = max([answer.index(i) for i in click])\n",
    "                [examines.append(i) for i in answer[:last_click+1]]\n",
    "            except:\n",
    "                error = True\n",
    "    examines_counter = Counter(examines)\n",
    "    try:\n",
    "        click_counter = Counter(list(itertools.chain(*query_clicks[q])))\n",
    "    except:\n",
    "        click_counter = Counter(query_clicks[q])\n",
    "    #now we want to divide the clicks by the examines to get the attractive parameter\n",
    "    query_alphas[q] = dict(Counter({k:click_counter[k]/examines_counter[k] for k in examines_counter}))\n",
    "\n",
    "#now we do the same trick as before as we cannot link documents/queries. So we pick three categories.\n",
    "alphas = []\n",
    "for key in query_alphas:\n",
    "    [alphas.append(i) for i in list(query_alphas[key].values())]\n",
    "alphas = np.sort(alphas)\n",
    "pairs = len(alphas)\n",
    "\n",
    "alpha_dict = {'N': np.mean(alphas[:int(pairs/3)]), 'R': np.mean(alphas[int(pairs/3):-int(pairs/3)]), 'HR': np.mean(alphas[-int(pairs/3):])}\n",
    "print(alpha_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Simulate Interleaving Experiment (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run experiment n times with balanced interleaving and the random click model\n",
    "def run_interleaving_rcm(N,interleaving):\n",
    "    \n",
    "    # Initialize random click model first\n",
    "    documents,_,clicks = read_data()\n",
    "    rho = rcm(documents,clicks)\n",
    "    \n",
    "    A_winner = 0\n",
    "    B_winner = 0\n",
    "    Tie = 0\n",
    "        \n",
    "    for combination in combinations:\n",
    "        if(interleaving == 1):\n",
    "            I,I_ids,rank_A,rank_B = balanced_interleaving(combination,True)\n",
    "        elif(interleaving == 2):\n",
    "            I,I_ids,rank_A,rank_B = team_based_interleaving(combination,True)\n",
    "        else:\n",
    "            print(\"Please use 1 for balanced interleaving and 2 for team based interleaving.\")\n",
    "        # The cutoff bool determines the lenth of the interleaved list\n",
    "        # If set to false, the length is rank_A + rank_B - 1\n",
    "        # Is set to true, the length is of rank_A\n",
    "        \n",
    "        # We ignore order since it is a stochastic process anyway  \n",
    "        for n in range(0,N): #We decided to keep the interleaved list equal across the N trials.\n",
    "            clicks = []\n",
    "            for i in range(0,len(I)):\n",
    "                random_variable = np.random.uniform()\n",
    "\n",
    "                if random_variable < rho:\n",
    "                    clicks.append(True)\n",
    "                else:\n",
    "                    clicks.append(False)\n",
    "\n",
    "            winner = define_winner(clicks,I_ids)\n",
    "\n",
    "            if winner == \"A\":\n",
    "                A_winner += 1\n",
    "            elif winner == \"B\":\n",
    "                B_winner += 1\n",
    "            else:\n",
    "                Tie += 1\n",
    "            \n",
    "        \n",
    "    total = A_winner + B_winner + Tie    \n",
    "    print (\"Percentage of cases that E outperforms P:\",round(100*A_winner/total,2),\"and percentage of ties is: \",round(100*Tie/total,2))\n",
    "    #print (\"P outperforms E\",100*B_winner/total, \"percent of the time\")\n",
    "    #print (\"It is a tie\",100*Tie/total, \"percent of the time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run experiment n times with balanced interleaving and the SDB click model\n",
    "def run_interleaving_SDBM(sigma_dict, alpha_dict,N,interleaving):\n",
    "    A_winner = 0\n",
    "    B_winner = 0\n",
    "    Tie = 0\n",
    "        \n",
    "    for combination in combinations:\n",
    "        if(interleaving == 1):\n",
    "            I,I_ids,rank_A,rank_B = balanced_interleaving(combination,True)\n",
    "        elif(interleaving == 2):\n",
    "            I,I_ids,rank_A,rank_B = team_based_interleaving(combination,True)\n",
    "        else:\n",
    "            print(\"Please use 1 for balanced interleaving and 2 for team based interleaving.\")\n",
    "        # The cutoff bool determines the lenth of the interleaved list\n",
    "        # If set to false, the length is rank_A + rank_B - 1\n",
    "        # Is set to true, the length is of rank_A\n",
    "        \n",
    "        for n in range(0,N):\n",
    "            clicks = []\n",
    "            satisfied = False\n",
    "\n",
    "            for i in range(0,len(I)):\n",
    "                relevance = I[i]\n",
    "                random_variable = np.random.uniform()\n",
    "\n",
    "                if random_variable < alpha_dict[relevance]: #here we check if the user is attracted\n",
    "                    if len(clicks) < len(I):\n",
    "                        clicks.append(True)\n",
    "                    random_variable_2 = np.random.uniform()\n",
    "                    if random_variable_2 < sigma_dict[relevance]: #here we look if the user was satisfied\n",
    "                        while len(clicks) < len(I):\n",
    "                            clicks.append(False) #if the user was satisfied, he does not click the rest of the list.\n",
    "                else:\n",
    "                    if len(clicks) < len(I):\n",
    "                        clicks.append(False)\n",
    "\n",
    "            winner = define_winner(clicks,I_ids)\n",
    "\n",
    "            if winner == \"A\":\n",
    "                A_winner += 1\n",
    "            elif winner == \"B\":\n",
    "                B_winner += 1\n",
    "            else:\n",
    "                Tie += 1\n",
    "\n",
    "        \n",
    "    total = A_winner + B_winner + Tie    \n",
    "    print (\"Percentage of cases that E outperforms P:\",round(100*A_winner/total,2),\"and percentage of ties is: \",round(100*Tie/total,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Results and Analysis (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline evaluation: \n",
      "Percentage of cases that E outperforms P with average precision measure: 0.48  and percentage of ties is:  0.05\n",
      "Percentage of cases that E outperforms P with ERR measure: 0.5 and percentage of ties is:  0.0\n",
      "Percentage of cases that E outperforms P with NDCG measure: 0.5 and percentage of ties is:  0.0\n",
      "\n",
      "Online evaluation with random click model and balanced interleaving: \n",
      "Percentage of cases that E outperforms P: 22.15 and percentage of ties is:  55.74\n",
      "\n",
      "Online evaluation with random click model and team based interleaving: \n",
      "Percentage of cases that E outperforms P: 22.09 and percentage of ties is:  55.77\n",
      "\n",
      "Online evaluation with SDBM and balanced interleaving: \n",
      "Percentage of cases that E outperforms P: 32.85 and percentage of ties is:  34.28\n",
      "\n",
      "Online evaluation with SDBM and team based interleaving: \n",
      "Percentage of cases that E outperforms P: 32.73 and percentage of ties is:  34.36\n"
     ]
    }
   ],
   "source": [
    "print(\"Offline evaluation: \")\n",
    "ap,err,ndcg,total_ap_e,total_ap_p,total_err_e,total_err_p,total_ndcg_e,total_ndcg_p = check_performance(combinations)\n",
    "print(\"\\nOnline evaluation with random click model and balanced interleaving: \")\n",
    "#Random click model has a lot of ties bacause often there are 0 clicks and we can also have both 1 or 2 clicks.\n",
    "run_interleaving_rcm(5,1)\n",
    "print(\"\\nOnline evaluation with random click model and team based interleaving: \")\n",
    "#Random click model has a lot of ties bacause often there are 0 clicks and we can also have both 1 or 2 clicks.\n",
    "run_interleaving_rcm(5,2)\n",
    "print(\"\\nOnline evaluation with SDBM and balanced interleaving: \")\n",
    "run_interleaving_SDBM(sigma_dict, alpha_dict,5,1)\n",
    "print(\"\\nOnline evaluation with SDBM and team based interleaving: \")\n",
    "run_interleaving_SDBM(sigma_dict, alpha_dict,5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=0.0, pvalue=1.0)\n",
      "Ttest_indResult(statistic=-3.0655144714229421e-13, pvalue=0.99999999999975542)\n",
      "Ttest_indResult(statistic=9.4888429388964605e-14, pvalue=0.99999999999992428) \n",
      "\n",
      "The delta average precision when E outperformed P has a mean of 0.251985571156 a standard deviation of 0.177610579289 and the maximum and minimum are 1.0 and 0.0047619047619 .\n",
      "\n",
      "The delta ERR when E outperformed P has a mean of 0.138850591743 a standard deviation of 0.108216781117 and the maximum and minimum are 0.429455566406 and 3.05175781246e-06 .\n",
      "\n",
      "The delta NDCG when E outperformed P has a mean of 0.28219244365 a standard deviation of 0.202560648236 and the maximum and minimum are 1.0 and 6.70863437456e-05 .\n"
     ]
    }
   ],
   "source": [
    "#Visualization results\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "print(stats.ttest_ind(total_ap_e,total_ap_p))\n",
    "print(stats.ttest_ind(total_err_e,total_err_p))\n",
    "print(stats.ttest_ind(total_ndcg_e,total_ndcg_p),\"\\n\")\n",
    "\n",
    "print(\"The delta average precision when E outperformed P has a mean of\", np.mean(ap), \"a standard deviation of\", np.std(ap), \"and the maximum and minimum are\", np.max(ap), \"and\", np.min(ap),\".\\n\")\n",
    "\n",
    "print(\"The delta ERR when E outperformed P has a mean of\", np.mean(err), \"a standard deviation of\",np.std(err), \"and the maximum and minimum are\", np.max(err), \"and\", np.min(err),\".\\n\")\n",
    "\n",
    "print(\"The delta NDCG when E outperformed P has a mean of\", np.mean(ndcg), \"a standard deviation of\",np.std(ndcg), \"and the maximum and minimum are\", np.max(ndcg), \"and\", np.min(ndcg),\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Conclusion offline and online evaluation methods</b>\n",
    "When looking at the outcome of the offline and online evaluation methods, we notice a few interesting things. First of all, we see a clear difference between online and offline evaluation in the number of ties. In all three offline evaluation methods the number of ties is very small, while with online evaluation this is between 35 and 55%. This is caused by the fact that in offline evaluation the probability of a draw is very small as the values are real numbers. The probability of a draw with online evaluation is much larger since this happens whenever both E and P get the same amount of clicks. Which especially often happens when we have 0 clicks in total. That is also the reason why when using the random click model the number of draws is largest, as there the probability of a click is only around 13%, whihch causes a lot of cases where we have 0 clicks for both models and thus a tie. This larger number of ties in online evaluation methods causes that we will have more cases where E outperforms P with only a very small (non significant) difference. Therefore, with online evaluation we need to perform more testing before we can accept a new version over an older version compared to when offline evaluation is used. On the other hand, the differences in offline evaluation can be very small, so also here we need significance tests to determine whether E really is better than P."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Conclusion interleaving methods</b>\n",
    "Although the assignment only asks for one interleaving method, we've implemented both team based and balanced interleaving. We see no clear difference in the number of times E outperforms P for both interleaving methods. This is the case since,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Conclusion click models</b>\n",
    "We see a clear difference between the random click model and the simplified dynamic Bayesian model. When the random click model is used, we have a large number of ties (which is explained before) therefore only in around 22.5% of the cases does either E outperform P or otherwise. With the SDBM we see a fair distribution between a win for E, a draw and a win for P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
