{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental part\n",
    "\n",
    "1) Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the next section, we first create a list of all combinations of relevances. We use itertools.product which gives all possible combinations of a list in any order. Then we use permutation which gives all the combinations of experiment and production relevances. We use itertools.permutations to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "values = ['N','R','HR'] #possible values of a prediction\n",
    "\n",
    "relevances = [] #relevances contains all combinations of N/R/HR with length 5\n",
    "for r in itertools.product(values, repeat=5):\n",
    "    relevances.append(list(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinations = [] #combinations contains all pairs of relevances\n",
    "for p in itertools.permutations(relevances, 2):\n",
    "    combinations.append(list(p)) #we use this to get rid of the permutations object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'N', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'R', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'R']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'N', 'HR', 'HR']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'N']], [['N', 'N', 'N', 'N', 'N'], ['N', 'N', 'R', 'N', 'R']]]\n"
     ]
    }
   ],
   "source": [
    "print(combinations[:10]) #show the first 10 combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Step 2: Implement Evaluation Measures (10 points)\n",
    "\n",
    "In the next section we take two assumptions:\n",
    "\n",
    "1) Values for the prediction relevances are N=0, R=1, HR=2\n",
    "\n",
    "2) The amount of relevant predictions (overall) is assumed to be the total amount of relevant (R or HR) docs in the prediction set. So we assume there is no overlap between articles in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "#the first binary evaluation methods: average precision\n",
    "numeric_map = {'N':0, 'R':1, 'HR':2} #we use this numeric map to map N/R/HR to a numeric value.\n",
    "prediction = ['R','HR','N','R','N'] #this is a sample prediction to test functions\n",
    "\n",
    "def count_rel(prediction1,prediction2):\n",
    "    return sum(1 for i in prediction1 if i != 'N') + sum(1 for i in prediction2 if i != 'N')\n",
    "\n",
    "def average_precision(prediction, r):\n",
    "    ap = 0\n",
    "    relevant_preds = 0\n",
    "    for i in range(0,len(prediction)):\n",
    "        if prediction[i] != 'N':\n",
    "            relevant_preds += 1\n",
    "            ap += relevant_preds/(i+1)\n",
    "    return ap/r\n",
    "\n",
    "ap = average_precision(prediction, count_rel(prediction, prediction))\n",
    "print(ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement both multi-graded evaluation methods. \n",
    "\n",
    "The first is nDCG@k which requires a optimal prediction to normalize predictions. Here we will use the total amount of HR/R files to create an optimum prediction. Again we assume there is no overlap in predictions.\n",
    "\n",
    "The second one is ERR, this model does not need any assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53641800576\n"
     ]
    }
   ],
   "source": [
    "#nDCG@K\n",
    "import numpy as np #Numpy is amazing right?\n",
    "\n",
    "def generate_opt(prediction1, prediction2): #generate optimal sequences from two predictions\n",
    "    opt_pred = []\n",
    "    num_hr = sum(1 for i in prediction1 if i == 'HR') + sum(1 for i in prediction2 if i == 'HR')\n",
    "    num_r  = sum(1 for i in prediction1 if i == 'R') + sum(1 for i in prediction2 if i == 'R')\n",
    "    for i in range(min(num_hr,5)): #check if num_hr exceeds 5, fill with HR's\n",
    "        opt_pred.append('HR')\n",
    "    for i in range(min(5-num_hr,num_r)): #check if num_r exceeds the space left, will with R's\n",
    "        opt_pred.append('R')\n",
    "    for i in range(5-len(opt_pred)): #fill the rest with N\n",
    "        opt_pred.append('N')\n",
    "    return opt_pred\n",
    "\n",
    "def dcg_k(numeric_map, prediction, opt_pred, k):\n",
    "    dcg_opt = 0\n",
    "    dcg = 0\n",
    "    for i in range(0,k): #for the range until K, we sum both the optimum and prediction dcg\n",
    "        dcg_opt += (2**numeric_map[opt_pred[i]]-1)/np.log2(1+i+1)\n",
    "        dcg +=(2**numeric_map[prediction[i]]-1)/np.log2(1+i+1)\n",
    "    return dcg/dcg_opt #dcg is normalized compared to the optimum\n",
    "ndcg = dcg_k(numeric_map, prediction, generate_opt(prediction,prediction), 3)\n",
    "print(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.281982421875\n"
     ]
    }
   ],
   "source": [
    "#ERR\n",
    "def ERR(numeric_map, prediction):\n",
    "    err = 0\n",
    "    max_val = 2**max(list(numeric_map.values()))\n",
    "    thetas = [(2**numeric_map[p]-1)/max_val for p in prediction]\n",
    "    for i in range(0,len(prediction)):\n",
    "        prod_val = 1\n",
    "        for j in range(0,i):\n",
    "            prod_val *= (1-thetas[j])*thetas[i]\n",
    "        prod_val *= 1/(i+1)\n",
    "        err += prod_val\n",
    "    return err\n",
    "err = ERR(numeric_map, prediction)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Step 3: Calculate the 𝛥measure (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'N', 'N', 'N', 'R'] ['HR', 'N', 'N', 'N', 'N']\n",
      "The average prec. scores are  0.4666666666666666 0.3333333333333333  for experiment and production respectively!\n",
      "The ERR scores are  1.0005859375 1.0  for experiment and production respectively!\n",
      "The NDCG scores @ k= 5  are:  0.33572413233 0.726228761795  for experiment and production respectively!!!\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "def check_performance(s):\n",
    "    prediction_e = s[0]\n",
    "    prediction_p = s[1]\n",
    "    print(prediction_e, prediction_p)\n",
    "    r = count_rel(prediction_e, prediction_p)\n",
    "    ap_e, ap_p = average_precision(prediction_e, r), average_precision(prediction_p, r)\n",
    "    print('The average prec. scores are ',ap_e,ap_p,' for experiment and production respectively!')\n",
    "    ERR_e, ERR_p = ERR(numeric_map, prediction_e), ERR(numeric_map, prediction_p)\n",
    "    print('The ERR scores are ',ERR_e,ERR_p,' for experiment and production respectively!')\n",
    "    opt_prediction = generate_opt(prediction_e,prediction_p)\n",
    "    ndcg_e = dcg_k(numeric_map, prediction_e, opt_prediction, k)\n",
    "    ndcg_p = dcg_k(numeric_map, prediction_p, opt_prediction, k)\n",
    "    print('The NDCG scores @ k=',k,' are: ',ndcg_e, ndcg_p,' for experiment and production respectively!!!')\n",
    "\n",
    "check_performance(combinations[20005])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Implement Interleaving (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_A_first(): #This is a function that determines is ranking A goes first or not\n",
    "    A = np.random.uniform() # Take a random uniform number between 0 and 1    \n",
    "    if A > 0.5: \n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def balanced_interleaving(s):\n",
    "    \n",
    "    ranking_A = s[0]\n",
    "    ranking_B = s[1]\n",
    "    \n",
    "    #print (\"ranking A is\",ranking_A)\n",
    "    #print (\"ranking B is\",ranking_B)\n",
    "    \n",
    "    # Initialize\n",
    "    I = []\n",
    "    k_a, k_b = 0,0\n",
    "        \n",
    "    A_first = get_A_first() #Find out if A or B goes first\n",
    "    \n",
    "    # We assume that rankA and rankB contain 10 unique documents\n",
    "    # That is why we can cast rankA and rankB to a dict\n",
    "    # This makes it easier to return a list of length 9, while adhering to the pseudo code from the slides\n",
    "    \n",
    "    rankA = {}\n",
    "    rankB = {}\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]\n",
    "    \n",
    "    for i in A:\n",
    "        rankA[i] = ranking_A[i]\n",
    "        \n",
    "    for j in B:\n",
    "        rankB[j] = ranking_B[j-5] \n",
    "        \n",
    "    # This code just follows the pseudo code from the slides\n",
    "    while k_a+1 <= len(ranking_A) and k_b+1 <= len(ranking_B):\n",
    "        if (k_a < k_b) or ((k_a == k_b) and A_first):\n",
    "            if A[k_a] not in I:\n",
    "                I.append(A[k_a])\n",
    "            k_a += 1\n",
    "            \n",
    "        else:\n",
    "            if B[k_b] not in I:\n",
    "                I.append(B[k_b])\n",
    "            k_b += 1\n",
    "             \n",
    "    # I is now filled with unique indices, we now have to convert these back to labels\n",
    "    \n",
    "    I_ids = I\n",
    "    \n",
    "    for i in range(0,len(I)):\n",
    "        try:\n",
    "            I[i] = rankA[I[i]]\n",
    "        except:\n",
    "            I[i] = rankB[I[i]]\n",
    "                    \n",
    "    return I, I_ids, ranking_A, ranking_B\n",
    "\n",
    "def define_winner(clicks,ranking_A,ranking_B):\n",
    "            \n",
    "    clicks_A = 0 # Number of clicks from result A\n",
    "    clicks_B = 0 # Number or clicks from result B\n",
    "    \n",
    "    A = [i for i in range(0,5)]\n",
    "    B = [i for i in range(5,10)]        \n",
    "    \n",
    "    for click in clicks: # Loop over the clicks\n",
    "        if click in A:\n",
    "            clicks_A += 1\n",
    "        elif click in B:\n",
    "            clicks_B += 1\n",
    "            \n",
    "    if clicks_A > clicks_B:\n",
    "        return \"A\"\n",
    "    elif clicks_B > clicks_A:\n",
    "        return \"B\"\n",
    "    else: \n",
    "        return \"Tie\"\n",
    "            \n",
    "test_set = combinations[12347]\n",
    "clicks = [1,6,7]\n",
    "\n",
    "I, I_ids, rank_A, rank_B = balanced_interleaving(test_set)\n",
    "#define_winner(clicks,rank_A,rank_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Step 5: Implement User Clicks Simulation (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59, 89, 29, 61, 25, 2, 63, 42, 94, 71], [2, 394, 59, 89, 29, 94, 867, 876, 42, 377], [29, 61, 94, 13, 53884, 3501, 850, 53882, 53885, 53883], [59, 56888, 56872, 293, 56876, 56889, 56875, 56877, 7697, 16552]]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#read search query data\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def read_data():\n",
    "\n",
    "    answers = []\n",
    "    query_ids = []\n",
    "    clicks = []\n",
    "    click = []\n",
    "    last_type = 'C'\n",
    "\n",
    "    with open('YandexRelPredChallenge.txt') as f:\n",
    "        for line in f:\n",
    "            vals = re.split(r'\\t+', line.rstrip())\n",
    "            line_type = vals[2] #we look at the type of data line\n",
    "            if line_type == 'Q': #if type is query, we append the query.\n",
    "                if len(click) > 0: #we append clicks of last query before we go further\n",
    "                    clicks.append(click)\n",
    "                    click = []\n",
    "                answers.append(list(map(int, vals[5:])))\n",
    "                query_ids.append(int(vals[3]))\n",
    "            if last_type == 'Q' and line_type == 'Q': #If last type also was query there are no clicks  \n",
    "                clicks.append([])\n",
    "            elif line_type == 'C':\n",
    "                click.append(int(vals[3]))\n",
    "            last_type = vals[2]\n",
    "        clicks.append(click) #we shall not forget the last click sequence...\n",
    "    \n",
    "    return answers,query_ids,clicks\n",
    "\n",
    "answers,query_ids,clicks = read_data()\n",
    "    \n",
    "'''\n",
    "print('Some sample answers and clicks:')\n",
    "print(query_ids[:5])\n",
    "print(answers[:5])\n",
    "print(clicks[:5])\n",
    "print('We have ',len(clicks),' answers/click sequences in total!')\n",
    "'''\n",
    "\n",
    "doubles_list = []\n",
    "for i in range(0,len(query_ids)):\n",
    "    if query_ids[i] == 0:\n",
    "        if answers[i] not in doubles_list:\n",
    "            doubles_list.append(answers[i])\n",
    "\n",
    "print(doubles_list)\n",
    "print (len(doubles_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Function to determine rho parameter of random click model given set of documents and clicks\n",
    "def rcm(documents,clicks):\n",
    "    unique_docs = []\n",
    "    unique_clicks = []\n",
    "    \n",
    "    assert(len(documents) == len(clicks))\n",
    "    \n",
    "    #First we find all unique documents to get the count\n",
    "    for d in documents:\n",
    "        for e in d:\n",
    "            unique_docs.append(e)\n",
    "\n",
    "    number_of_docs = len(Counter(unique_docs).keys())\n",
    "    \n",
    "    #Now we determine for the total number of cliks\n",
    "    for c in clicks:\n",
    "        for e in c:\n",
    "            unique_clicks.append(e)\n",
    "        \n",
    "    number_of_clicks = len(Counter(unique_clicks).keys())\n",
    "    \n",
    "    rho = number_of_clicks/number_of_docs\n",
    "    \n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [59, 89, 29, 61, 25, 2, 63, 42, 94, 71], 1: [15, 32, 0, 9, 80, 30, 88, 82, 97, 65], 2: [72, 11, 102, 83, 96, 79, 22, 38, 68, 91], 3: [19, 44, 5, 53, 41, 56, 58, 40, 3, 31], 4: [45, 20, 24, 23, 34, 33, 4, 90, 35, 60], 5: [99, 16, 87, 39, 100, 6, 36, 52, 81, 84], 6: [46, 49, 28, 78, 106, 14, 37, 48, 17, 10], 7: [77, 93, 55, 86, 64, 67, 76, 98, 18, 54], 8: [7, 103, 51, 92, 43, 12, 73, 69, 27, 105], 9: [13, 70, 66, 94, 50, 104, 29, 21, 89, 85]}\n"
     ]
    }
   ],
   "source": [
    "#Dynamic Bayesian network model\n",
    "#first we will look at sigma, as we can derive this by MLE directly\n",
    "from operator import itemgetter\n",
    "query_answers = {}\n",
    "query_clicks = {}\n",
    "query_last_clicks = {}\n",
    "query_sigma = {}\n",
    "for q in np.unique(query_ids)[:10]:\n",
    "    indices = [i for i, j in enumerate(query_ids) if j == q]\n",
    "    query_answers[q] = answers[indices[0]]\n",
    "    query_clicks[q] = list(itemgetter(*indices)(clicks))\n",
    "    last_clicks = []\n",
    "    #print(q, query_clicks[q])\n",
    "    for i in query_clicks[q]:\n",
    "        if isinstance(i, int):\n",
    "            last_clicks.append(query_clicks[q][-1])\n",
    "            break\n",
    "        elif len(i) > 0:\n",
    "            last_clicks.append(i[-1])\n",
    "        else:\n",
    "            last_clicks.append(0)\n",
    "    query_last_clicks[q] = last_clicks\n",
    "    \n",
    "print (query_answers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Initialize random click model first\n",
    "    documents,_,clicks = read_data()\n",
    "    rho = rcm(documents,clicks)\n",
    "    \n",
    "    A_winner = 0\n",
    "    B_winner = 0\n",
    "    Tie = 0\n",
    "        \n",
    "    for combination in combinations:\n",
    "        I,I_ids,rank_A,rank_B = balanced_interleaving(combination)\n",
    "                \n",
    "        # We ignore order since it is a stochastic process anyway\n",
    "        \n",
    "        clicks = []\n",
    "        \n",
    "        for i in range(0,len(I)):\n",
    "            random_variable = np.random.uniform()\n",
    "            \n",
    "            if random_variable < rho:\n",
    "                clicks.append(i)    \n",
    "                \n",
    "        winner = define_winner(clicks,rank_A,rank_B)\n",
    "        \n",
    "        if winner == \"A\":\n",
    "            A_winner += 1\n",
    "        elif winner == \"B\":\n",
    "            B_winner += 1\n",
    "        else:\n",
    "            Tie += 1\n",
    "        \n",
    "    total = A_winner + B_winner + Tie    \n",
    "    print (\"A wins\",100*A_winner/total, \"percent of the time\")\n",
    "    print (\"B wins\",100*B_winner/total, \"percent of the time\")\n",
    "    print (\"It is a tie\",100*Tie/total, \"percent of the time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wins 36.20208822229024 percent of the time\n",
      "B wins 26.102778628031153 percent of the time\n",
      "It is a tie 37.695133149678604 percent of the time\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
